import argparse
import json
import pandas as pd
import time
from datetime import datetime, timedelta
from pathlib import Path
from requests.exceptions import SSLError

from config_and_brand import engine, API_KEY, MODEL, brand_profile
from AI_prompt import call_kimi_api, safe_dumps
from summarize import summarize
from cpc_analysis import compute_cpc_contribution_ratios
import matplotlib.pyplot as plt
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils import get_column_letter


# å¦‚æœæœ¬åœ°æ²¡æœ‰ ace_toolsï¼Œå°±å®šä¹‰ä¸€ä¸ªç®€å•çš„ fallback

# === æ—¥æŠ¥è¾“å‡ºé…ç½® ===
THRESHOLD = 30   # æš´æ¶¨/æš´è·Œåˆ¤å®šé˜ˆå€¼ %
CORE_FIELDS = ["æ¶ˆè´¹é‡‘é¢", "æ‰“å¡äººæ•°", "æ–°å¢æ”¶è—äººæ•°", "æ–°å¥½è¯„æ•°", "æ–°ä¸­å·®è¯„æ•°"]
RANK_PLATFORMS = {
    "dianping_hot": "ç‚¹è¯„çƒ­é—¨æ¦œ",
    "dianping_checkin": "ç‚¹è¯„æ‰“å¡äººæ°”æ¦œ",
    "dianping_rating":"ç‚¹è¯„å¥½è¯„æ¦œ"
}

try:
    from ace_tools import display_dataframe_to_user
except ImportError:
    def display_dataframe_to_user(title, df):
        print(f"\n=== {title} ===\n")
        print(df.to_string())
        # å¦‚éœ€ä¿å­˜åˆ° CSVï¼Œå¯æ”¹ä¸ºï¼š
        # df.to_csv(f"./daily_report/{title}.csv", encoding="utf-8-sig", index=True)

from matplotlib import font_manager
# æ³¨å†Œé¡¹ç›®å†…çš„ SimHei.ttf
font_manager.fontManager.addfont("./fonts/SimHei.ttf")
plt.rcParams['font.family'] = 'SimHei'
plt.rcParams['axes.unicode_minus'] = False

# å…¨å±€åŠ è½½é—¨åº—æ˜ å°„ï¼Œplot_comparison_table å¯ä»¥ç›´æ¥ç”¨
store_map = pd.read_sql(
    "SELECT é—¨åº—ID AS store_id, ç¾å›¢é—¨åº—ID, æ¨å¹¿é—¨åº— AS brand_name, è¿è¥å¸ˆ AS operator FROM store_mapping",
    engine
)

def generate_weekly_comparison_table(brand, report_date):
    # å†…éƒ¨ç›´æ¥ç”¨å…¨å±€ engineã€store_map
    """
    ç”Ÿæˆå“ç‰Œå½“æ—¥ vs ä¸Šå‘¨åŒæœŸï¼ˆå¦‚å·¥ä½œæ—¥å¯¹æ¯”åŒweekdayï¼Œä¸Šå‘¨æœ«ä¸‰å¤©å¯¹æ¯”ï¼‰å¯¹æ¯”è¡¨æ ¼ï¼Œ
    æŒ‡æ ‡åŒ…å«ï¼šæ›å…‰äººæ•°, è®¿é—®äººæ•°, è´­ä¹°äººæ•°, æ¶ˆè´¹é‡‘é¢,
    æ–°å¥½è¯„æ•°, æ–°ä¸­å·®è¯„æ•°, æ‰“å¡äººæ•°, æ‰«ç äººæ•°, æ–°å¢æ”¶è—äººæ•°, ç‚¹è¯„æ˜Ÿçº§
    """
    metrics = [
        "æ›å…‰äººæ•°","è®¿é—®äººæ•°","è´­ä¹°äººæ•°","æ¶ˆè´¹é‡‘é¢",
        "æ–°å¥½è¯„æ•°","æ–°ä¸­å·®è¯„æ•°","æ‰“å¡äººæ•°","æ‰«ç äººæ•°","æ–°å¢æ”¶è—äººæ•°","ç‚¹è¯„æ˜Ÿçº§"
    ]
    # 1) MeiTuan IDs for brand
    mids = store_map.loc[store_map['brand_name']==brand, 'ç¾å›¢é—¨åº—ID'].astype(str).tolist()
    if not mids:
        print(f"âš ï¸ å“ç‰Œ {brand} æ— å¯¹åº”ç¾å›¢é—¨åº—IDï¼Œè·³è¿‡")
        return
    mids_sql = ",".join(f"'{m}'" for m in mids)

    # 2) Determine current and last-week dates
    wd = report_date.weekday()  # 0=Mon, ...,6=Sun
    if wd == 0:
        # Monday: compare last Fri-Sun vs Fri-Sun
        curr_start = report_date - timedelta(days=3)
        prev_start = report_date - timedelta(days=10)
        current_dates = [d.date() for d in pd.date_range(curr_start, periods=3)]
        prev_dates    = [d.date() for d in pd.date_range(prev_start, periods=3)]
    else:
        # Tue-Fri: compare that weekday
        current_dates = [report_date.date()]
        prev_dates    = [(report_date - timedelta(days=7)).date()]

    # 3) Load data
    cols = ", ".join(f"`{m}`" for m in metrics)
    cd_sql = ",".join(f"'{d}'" for d in current_dates)
    pd_sql = ",".join(f"'{d}'" for d in prev_dates)
    df_curr = pd.read_sql(
        f"SELECT `æ—¥æœŸ`, {cols} FROM `operation_data` "
        f"WHERE `ç¾å›¢é—¨åº—ID` IN ({mids_sql}) AND `æ—¥æœŸ` IN ({cd_sql}) "
        f"ORDER BY `æ—¥æœŸ`",
        engine
    ).set_index('æ—¥æœŸ')
    df_prev = pd.read_sql(
        f"SELECT `æ—¥æœŸ`, {cols} FROM `operation_data` "
        f"WHERE `ç¾å›¢é—¨åº—ID` IN ({mids_sql}) AND `æ—¥æœŸ` IN ({pd_sql}) "
        f"ORDER BY `æ—¥æœŸ`",
        engine
    ).set_index('æ—¥æœŸ')
    df_prev = df_prev.add_prefix('ä¸Šå‘¨_')
    df_curr = df_curr.add_prefix('æœ¬å‘¨_')

    # 4) Combine and compute change rates
    df_cmp = pd.concat([df_curr, df_prev], axis=1)
    for m in metrics:
        curr_col = f"æœ¬å‘¨_{m}"
        prev_col = f"ä¸Šå‘¨_{m}"
        df_cmp[f"{m}_å˜åŒ–ç‡"] = (
            (df_cmp[curr_col] - df_cmp[prev_col]) / df_cmp[prev_col] * 100
        ).round(1).astype(str) + '%'

    # 5) Display interactive table
    display_dataframe_to_user(f"{brand} åŒæœŸå¯¹æ¯”è¡¨", df_cmp)

def plot_vertical_table(brand, report_date, engine):
    from datetime import timedelta

    # 1) å–æœ€è¿‘ 7 å¤©æ•°æ®ï¼ŒSQL é‡ŒæŠŠæ‰€æœ‰æŒ‡æ ‡éƒ½æ‹‰å‡ºæ¥
    start = report_date - timedelta(days=6)
    df = pd.read_sql(f"""
        SELECT 
            `æ—¥æœŸ`,
            `æ¶ˆè´¹é‡‘é¢` AS `æ¶ˆè´¹é‡‘é¢`,
            `æ›å…‰äººæ•°`,
            `è®¿é—®äººæ•°`,
            `è´­ä¹°äººæ•°`,
            `æ‰“å¡äººæ•°`,
            `æ‰«ç äººæ•°`,
            `æ–°å¢æ”¶è—äººæ•°`,
            `æ–°å¥½è¯„æ•°`,
            `æ–°ä¸­å·®è¯„æ•°`,
            `ç‚¹è¯„æ˜Ÿçº§`    AS `æ˜Ÿçº§`
        FROM `operation_data`
        WHERE `ç¾å›¢é—¨åº—ID` IN (
            SELECT `ç¾å›¢é—¨åº—ID` 
              FROM `store_mapping` 
             WHERE `æ¨å¹¿é—¨åº—` = '{brand}'
        )
          AND `æ—¥æœŸ` BETWEEN '{start.date()}' AND '{report_date.date()}'
        ORDER BY `æ—¥æœŸ`
    """, engine)
    if df.empty:
        print(f"âš ï¸ {brand} æœ€è¿‘7å¤©æ— æ•°æ®ï¼Œè·³è¿‡")
        return

    # 2) æŠŠæ—¥æœŸåˆ—è½¬æˆçº¯ YYYY-MM-DD çš„å­—ç¬¦ä¸²ï¼Œå»æ‰ â€œ00:00:00â€
    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ']).dt.strftime('%Y-%m-%d')

    # 3) è¡ç”Ÿæ˜ŸæœŸå‡ 
    weekday_map = {0:'æ˜ŸæœŸä¸€',1:'æ˜ŸæœŸäºŒ',2:'æ˜ŸæœŸä¸‰',3:'æ˜ŸæœŸå››',
                   4:'æ˜ŸæœŸäº”',5:'æ˜ŸæœŸå…­',6:'æ˜ŸæœŸæ—¥'}
    df['æ˜ŸæœŸ'] = pd.to_datetime(df['æ—¥æœŸ']).dt.weekday.map(weekday_map)

    # 4) å¼ºåˆ¶æŠŠæ‰€æœ‰â€œäººæ•°â€åˆ—å’Œè¯„åˆ†åˆ—å˜æˆæ•´æ•°
    int_cols = [
        'æ›å…‰äººæ•°','è®¿é—®äººæ•°','è´­ä¹°äººæ•°',
        'æ‰“å¡äººæ•°','æ‰«ç äººæ•°','æ–°å¢æ”¶è—äººæ•°',
        'æ–°å¥½è¯„æ•°','æ–°ä¸­å·®è¯„æ•°'
    ]
    df[int_cols] = df[int_cols].astype(int)

    # 5) è®¡ç®—ä¸¤æ®µè½¬åŒ–ç‡
    df['æ›å…‰-è®¿é—®è½¬åŒ–ç‡'] = (df['è®¿é—®äººæ•°'] / df['æ›å…‰äººæ•°'] * 100).round(1).astype(str) + '%'
    df['è®¿é—®-è´­ä¹°è½¬åŒ–ç‡'] = (df['è´­ä¹°äººæ•°'] / df['è®¿é—®äººæ•°'] * 100).round(1).astype(str) + '%'

    # 6) æœ€ç»ˆé€‰åˆ— & æ’åºï¼Œå¹¶é‡å‘½åä¸ºçŸ­æ ‡é¢˜
    cols = [
        'æ—¥æœŸ', 'æ˜ŸæœŸ', 'æ¶ˆè´¹é‡‘é¢', 'æ›å…‰äººæ•°', 'è®¿é—®äººæ•°', 'è´­ä¹°äººæ•°',
        'æ›å…‰-è®¿é—®è½¬åŒ–ç‡', 'è®¿é—®-è´­ä¹°è½¬åŒ–ç‡',
        'æ–°å¢æ”¶è—äººæ•°', 'æ‰“å¡äººæ•°',
        'æ–°å¥½è¯„æ•°', 'æ–°ä¸­å·®è¯„æ•°', 'æ‰«ç äººæ•°', 'æ˜Ÿçº§'
    ]
    df = df[cols].rename(columns={
        'æ¶ˆè´¹é‡‘é¢': 'æ¶ˆè´¹',
        'æ›å…‰äººæ•°': 'æ›å…‰',
        'è®¿é—®äººæ•°': 'è®¿é—®',
        'è´­ä¹°äººæ•°': 'è´­ä¹°',
        'æ›å…‰-è®¿é—®è½¬åŒ–ç‡': 'è®¿é—®è½¬åŒ–',
        'è®¿é—®-è´­ä¹°è½¬åŒ–ç‡': 'è´­ä¹°è½¬åŒ–',
        'æ–°å¢æ”¶è—äººæ•°': 'æ”¶è—',
        'æ‰“å¡äººæ•°': 'æ‰“å¡',
        'æ–°å¥½è¯„æ•°': 'å¥½è¯„',
        'æ–°ä¸­å·®è¯„æ•°': 'å·®è¯„',
        'æ‰«ç äººæ•°': 'æ‰«ç ',
        'æ˜Ÿçº§':'æ˜Ÿçº§'
    })

    # 7) ç»˜åˆ¶è¡¨æ ¼ï¼Œå®½åº¦è°ƒå¤§ï¼Œå¹¶æŒ‡å®šæ¯åˆ—ç›¸å¯¹å®½åº¦
    col_widths = []
    for c in df.columns:
        if c in ['æ—¥æœŸ', 'æ˜ŸæœŸ']:
            col_widths.append(0.08)
        elif c in ['æ¶ˆè´¹', 'æ›å…‰', 'è®¿é—®', 'è´­ä¹°', 'è®¿é—®è½¬åŒ–', 'è´­ä¹°è½¬åŒ–']:
            col_widths.append(0.06)
        else:
            col_widths.append(0.05)

    fig, ax = plt.subplots(figsize=(16, 0.6 * len(df) + 1))
    ax.axis('off')
    tbl = ax.table(
        cellText=df.values,
        colLabels=df.columns,
        colWidths=col_widths,
        cellLoc='center',
        loc='center'
    )
    tbl.auto_set_font_size(False)
    tbl.set_fontsize(11)
    tbl.scale(1, 1.2)

    ax.set_title(f"{brand} æœ€è¿‘7å¤©å…³é”®æŒ‡æ ‡", fontsize=16, pad=12)
    plt.tight_layout()
    out = f"./daily_report/{brand}_æœ€è¿‘7å¤©æŒ‡æ ‡è¡¨.png"
    fig.savefig(out, dpi=150, bbox_inches='tight')
    plt.close(fig)
    print(f"âœ… ä¿å­˜ï¼š{out}")



def plot_comparison_table(brand, report_date):
    """
    ç”Ÿæˆå“ç‰Œâ€œæœ¬æœŸ vs ä¸ŠæœŸâ€åŒæœŸå¯¹æ¯”è¡¨æ ¼å¹¶ä¿å­˜æˆ PNGï¼š
    - å‘¨ä¸€å¯¹æ¯”å‘¨äº”~å‘¨æ—¥ä¸‰ä¸ªå·¥ä½œæ—¥ï¼Œå‘¨äºŒ~å‘¨äº”å¯¹æ¯”ä¸Šå‘¨åŒæ—¥
    - æŒ‡æ ‡ï¼šæ›å…‰äººæ•°, è®¿é—®äººæ•°, è´­ä¹°äººæ•°, æ¶ˆè´¹é‡‘é¢,
      æ–°å¥½è¯„æ•°, æ–°ä¸­å·®è¯„æ•°, æ‰“å¡äººæ•°, æ‰«ç äººæ•°, æ–°å¢æ”¶è—äººæ•°, ç‚¹è¯„æ˜Ÿçº§
    """
    # 1) ä»å…¨å±€ store_map å–å‡ºæœ¬å“ç‰Œæ‰€æœ‰ç¾å›¢é—¨åº—ID
    mids = store_map.loc[store_map['brand_name'] == brand, 'ç¾å›¢é—¨åº—ID'].astype(str).tolist()
    if not mids:
        print(f"âš ï¸ å“ç‰Œ {brand} æ— å¯¹åº”ç¾å›¢é—¨åº—IDï¼Œè·³è¿‡å¯¹æ¯”è¡¨")
        return
    mids_sql = ",".join(f"'{m}'" for m in mids)

    # 2) è®¡ç®—â€œæœ¬æœŸâ€â€œä¸ŠæœŸâ€æ—¥æœŸ
    wd = report_date.weekday()  # 0=å‘¨ä¸€ â€¦ 6=å‘¨æ—¥
    if wd == 0:
        # å‘¨ä¸€ï¼šæœ¬æœŸ = å‘¨äº”~å‘¨æ—¥ï¼Œ ä¸ŠæœŸ = ä¸Šå‘¨å‘¨äº”~å‘¨æ—¥
        curr_start = report_date - timedelta(days=3)
        prev_start = report_date - timedelta(days=10)
        current_dates = [d.date() for d in pd.date_range(curr_start, periods=3)]
        prev_dates    = [d.date() for d in pd.date_range(prev_start, periods=3)]
        label_curr, label_prev = "æœ¬æœŸ(å‘¨äº”~å‘¨æ—¥)", "ä¸ŠæœŸ(ä¸Šå‘¨å‘¨äº”~å‘¨æ—¥)"
    else:
        # å‘¨äºŒ~å‘¨äº”ï¼šæœ¬æœŸ = æ˜¨æ—¥, ä¸ŠæœŸ = ä¸Šå‘¨åŒæœŸ
        current_dates = [(report_date - timedelta(days=1)).date()]
        prev_dates    = [(report_date - timedelta(days=8)).date()]
        label_curr, label_prev = "æ˜¨æ—¥", "ä¸Šå‘¨åŒæœŸ"

    # 3) æ‹‰å–æ•°æ®
    metrics = [
        "æ›å…‰äººæ•°","è®¿é—®äººæ•°","è´­ä¹°äººæ•°","æ¶ˆè´¹é‡‘é¢",
        "æ–°å¥½è¯„æ•°","æ–°ä¸­å·®è¯„æ•°","æ‰“å¡äººæ•°","æ‰«ç äººæ•°","æ–°å¢æ”¶è—äººæ•°","ç‚¹è¯„æ˜Ÿçº§"
    ]
    cols = ", ".join(f"`{m}`" for m in metrics)
    cd_sql = ", ".join(f"'{d}'" for d in current_dates)
    pd_sql = ", ".join(f"'{d}'" for d in prev_dates)

    df_curr = pd.read_sql(
        f"SELECT `æ—¥æœŸ`, {cols} FROM `operation_data` "
        f"WHERE `ç¾å›¢é—¨åº—ID` IN ({mids_sql}) AND `æ—¥æœŸ` IN ({cd_sql})",
        engine
    ).set_index("æ—¥æœŸ")

    df_prev = pd.read_sql(
        f"SELECT `æ—¥æœŸ`, {cols} FROM `operation_data` "
        f"WHERE `ç¾å›¢é—¨åº—ID` IN ({mids_sql}) AND `æ—¥æœŸ` IN ({pd_sql})",
        engine
    ).set_index("æ—¥æœŸ")

    # 4) æ±‡æ€»å¹¶æ„é€ å¯¹æ¯”è¡¨
    if df_curr.empty or df_prev.empty:
        print(f"âš ï¸ å“ç‰Œ {brand} æœ¬æœŸæˆ–ä¸ŠæœŸæ— æ•°æ®ï¼Œè·³è¿‡å¯¹æ¯”è¡¨")
        return
    curr_sum = df_curr.sum()
    prev_sum = df_prev.sum()
    df_cmp = pd.DataFrame({
        "æŒ‡æ ‡": metrics,
        label_curr: [curr_sum[m] for m in metrics],
        label_prev: [prev_sum[m] for m in metrics],
    })
    df_cmp["å˜åŒ–ç‡"] = (
        (df_cmp[label_curr] - df_cmp[label_prev]) / df_cmp[label_prev] * 100
    ).round(1).astype(str) + "%"

    # 5) å±•ç¤ºå¹¶ä¿å­˜
    display_dataframe_to_user(f"{brand} åŒæœŸå¯¹æ¯”è¡¨", df_cmp)

    fig, ax = plt.subplots(figsize=(len(df_cmp)*0.8, 2.5))
    ax.axis("off")
    tbl = ax.table(
        cellText=df_cmp.values,
        colLabels=df_cmp.columns,
        loc="center"
    )
    tbl.auto_set_font_size(False)
    tbl.set_fontsize(12)
    tbl.scale(1, 1.5)
    ax.set_title(brand, fontsize=16, pad=10)
    plt.tight_layout()
    out_path = f"./daily_report/{brand}_åŒæœŸå¯¹æ¯”è¡¨.png"
    fig.savefig(out_path, dpi=150, bbox_inches="tight")
    plt.close(fig)
    print(f"âœ… ä¿å­˜ {brand} åŒæœŸå¯¹æ¯”è¡¨ï¼š{out_path}")

def main():

    # ---------- CLI & æ—¥æœŸè®¡ç®— ----------
    parser = argparse.ArgumentParser()
    parser.add_argument("--date", help="æ—¥æŠ¥æ—¥æœŸï¼Œé»˜è®¤æ˜¨å¤© (YYYY-MM-DD)")
    args = parser.parse_args()

    # 1) å…ˆè®¡ç®—é»˜è®¤æ‹‰å–çš„æ—¥æœŸï¼ˆæ˜¨å¤©ï¼‰
    default_dt = datetime.now() - timedelta(days=1)
    default_str = default_dt.strftime("%Y-%m-%d")

    # 2) å¦‚æœå‘½ä»¤è¡Œç»™äº† --dateï¼Œå°±ç›´æ¥ç”¨ï¼›å¦åˆ™å¼¹äº¤äº’æç¤º
    if args.date:
        date_str = args.date
    else:
        # æç¤ºç”¨æˆ·ç¡®è®¤æˆ–è¾“å…¥
        inp = input(f"ä½¿ç”¨æ—¥æœŸ [{default_str}]ï¼Ÿå›è½¦ç¡®è®¤ æˆ– è¾“å…¥å…¶ä»–æ—¥æœŸ (YYYY-MM-DD)ï¼š").strip()
        date_str = inp if inp else default_str

    # 3) æœ€åè§£æ
    try:
        report_date = datetime.strptime(date_str, "%Y-%m-%d")
    except ValueError:
        print(f"âŒ æ—¥æœŸæ ¼å¼ä¸å¯¹ï¼š{date_str}ï¼Œè¯·æŒ‰ YYYY-MM-DD é‡è¯•")
        return

    last_7_start = report_date - timedelta(days=7)
    print(f"ğŸ—“ï¸ æœ€ç»ˆä½¿ç”¨æ—¥æœŸï¼š{report_date.date()}ï¼Œç”Ÿæˆæ—¥æŠ¥")

    # ---------- åœ¨å¼€å§‹å†™ TXT/Excel ä¹‹å‰ï¼Œç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ ----------
    out_dir = Path("./daily_report")
    out_dir.mkdir(exist_ok=True)

    # ---------- ä¸€æ¬¡æ€§æ‹‰å–æ‰€æœ‰æ•°æ® ----------
    print("â¡ï¸ æ‹‰å–æ˜¨æ—¥è¿è¥æ•°æ®")
    op_today = pd.read_sql(
        f"""SELECT 
               `ç¾å›¢é—¨åº—ID`,`æ›å…‰äººæ•°`,`è®¿é—®äººæ•°`,`è´­ä¹°äººæ•°`,
               `æ¶ˆè´¹é‡‘é¢`,`æ–°å¥½è¯„æ•°`,`æ–°ä¸­å·®è¯„æ•°`,
               `æ‰“å¡äººæ•°`,`æ‰«ç äººæ•°`,`ç‚¹è¯„æ˜Ÿçº§`,`æ–°å¢æ”¶è—äººæ•°`,`rankings_detail`
           FROM `operation_data`
           WHERE `æ—¥æœŸ` = '{report_date.date()}'""",
        engine
    ).merge(
        store_map[['ç¾å›¢é—¨åº—ID','brand_name','operator']],
        on='ç¾å›¢é—¨åº—ID', how='left'
    ).rename(columns={'brand_name':'æ¨å¹¿é—¨åº—'})

    print("â¡ï¸ æ‹‰å–è¿‘7å¤©è¿è¥æ•°æ®")
    op_last7 = pd.read_sql(
        f"""SELECT
               `ç¾å›¢é—¨åº—ID`,`æ—¥æœŸ`,`æ›å…‰äººæ•°`,`è®¿é—®äººæ•°`,`è´­ä¹°äººæ•°`,
               `æ¶ˆè´¹é‡‘é¢`,`æ–°å¥½è¯„æ•°`,`æ–°ä¸­å·®è¯„æ•°`,
               `æ‰“å¡äººæ•°`,`æ‰«ç äººæ•°`,`ç‚¹è¯„æ˜Ÿçº§`,`æ–°å¢æ”¶è—äººæ•°`
           FROM `operation_data`
           WHERE `æ—¥æœŸ` BETWEEN '{last_7_start.date()}' AND '{report_date.date()}'""",
        engine
    ).merge(
        store_map[['ç¾å›¢é—¨åº—ID','brand_name','operator']],
        on='ç¾å›¢é—¨åº—ID', how='left'
    ).rename(columns={'brand_name':'æ¨å¹¿é—¨åº—'})

    print("â¡ï¸ æ‹‰å–æ˜¨æ—¥CPCæ•°æ®")
    cpc_today = pd.read_sql(
        f"""SELECT `store_id`,`cost`,`impressions`,`clicks`,`orders`
           FROM `cpc_hourly_data`
           WHERE `date` = '{report_date.date()}'""",
        engine
    ).merge(
        store_map[['store_id','brand_name','operator']],
        on='store_id', how='left'
    ).rename(columns={'brand_name':'æ¨å¹¿é—¨åº—'})

    # ---------- åŠ è½½æœ€è¿‘ 14 å¤©å†å²æ•°æ®ï¼ˆç”¨äºç¯æ¯” & è¡¨æ ¼ï¼‰ ----------
    print("â¡ï¸ æ‹‰å–æœ€è¿‘14å¤©è¿è¥æ•°æ®")
    op_hist = pd.read_sql(
        f"""SELECT 
               `ç¾å›¢é—¨åº—ID`,`æ—¥æœŸ`,`æ›å…‰äººæ•°`,`è®¿é—®äººæ•°`,`è´­ä¹°äººæ•°`,
               `æ¶ˆè´¹é‡‘é¢`,`æ–°å¥½è¯„æ•°`,`æ–°ä¸­å·®è¯„æ•°`,
               `æ‰“å¡äººæ•°`,`æ‰«ç äººæ•°`,`ç‚¹è¯„æ˜Ÿçº§`,`æ–°å¢æ”¶è—äººæ•°`
           FROM `operation_data`
           WHERE `æ—¥æœŸ` BETWEEN '{(report_date - timedelta(days=14)).date()}' AND '{report_date.date()}'""",
        engine
    ).merge(
        store_map[['ç¾å›¢é—¨åº—ID','brand_name','operator']],
        on='ç¾å›¢é—¨åº—ID', how='left'
    ).rename(columns={'brand_name':'æ¨å¹¿é—¨åº—'})

    # ---------- åˆ†ç»„å‡†å¤‡ ----------
    op_group  = op_today.groupby("æ¨å¹¿é—¨åº—")
    op7_group = op_last7.groupby("æ¨å¹¿é—¨åº—")
    cpc_group = cpc_today.groupby("æ¨å¹¿é—¨åº—")

    # ---------- å…ˆä¸ºæ¯å®¶é—¨åº—ç”Ÿæˆå¯¹æ¯”å›¾è¡¨ ----------
    #for brand in op_group.groups.keys():
        #plot_vertical_table(brand, report_date, engine)

    # ---------- å†ä¸ºæ¯å®¶é—¨åº—ç”ŸæˆåŒæœŸå¯¹æ¯”è¡¨ ----------
    #for brand in op_group.groups.keys():
        #generate_weekly_comparison_table(brand, report_date)

    # ---------- é€šç”¨æ ¼å¼åŒ– ----------
    def is_big_change(pct_str, threshold=THRESHOLD):
        try:
            return abs(float(pct_str.strip('%'))) >= threshold
        except:
            return False

    def fmt(val, pct_str):
        if pct_str == "N/A":
            return str(val)
        pct = float(pct_str.strip('%'))
        if pct > 0:
            return f"{val}(+{pct}%)"
        if pct < 0:
            return f"{val}(-{abs(pct)}%)"
        return f"{val}(æŒå¹³)"

    from collections import defaultdict

    operator_sections = defaultdict(list)

    # ---------- æ„é€ æ¯å®¶é—¨åº—çš„ Section ----------
    sections = []
    for brand, df_op in op_group:
        # å‡†å¤‡å½“æ—¥7å¤©å†å² & CPC & å…¨é‡å†å²æ•°æ®
        df_op7    = op7_group.get_group(brand) if brand in op7_group.groups else pd.DataFrame()
        df_cpc    = cpc_group.get_group(brand)  if brand in cpc_group.groups  else pd.DataFrame()
        df_hist_b = op_hist[op_hist["æ¨å¹¿é—¨åº—"] == brand]

        # æŒ‡æ ‡å­—æ®µ
        op_fields  = ["æ›å…‰äººæ•°","è®¿é—®äººæ•°","è´­ä¹°äººæ•°","æ¶ˆè´¹é‡‘é¢",
                      "æ–°å¥½è¯„æ•°","æ–°ä¸­å·®è¯„æ•°","æ‰“å¡äººæ•°","æ‰«ç äººæ•°","ç‚¹è¯„æ˜Ÿçº§","æ–°å¢æ”¶è—äººæ•°"]
        cpc_fields = ["cost","impressions","clicks","orders"]

        # 1) æ±‡æ€»å½“æ—¥è¿è¥ & CPC æ•°æ®
        op_sum = summarize(df_op, op_fields)
        if df_cpc.empty:
            cpc_sum, ratios = {}, {}
        else:
            cpc_sum = summarize(df_cpc, cpc_fields)
            ratios  = compute_cpc_contribution_ratios(op_sum, cpc_sum)

        # 2) è®¡ç®—æ—¥ç¯æ¯”ï¼ˆæ˜¨æ—¥ vs. ä¸Šå‘¨åŒæœŸï¼‰
        weekday = report_date.weekday()
        if weekday == 0:
            curr = df_hist_b[
                (df_hist_b["æ—¥æœŸ"] >= (report_date - timedelta(days=3)).date()) &
                (df_hist_b["æ—¥æœŸ"] <= (report_date - timedelta(days=1)).date())
            ]
            prev = df_hist_b[
                (df_hist_b["æ—¥æœŸ"] >= (report_date - timedelta(days=10)).date()) &
                (df_hist_b["æ—¥æœŸ"] <= (report_date - timedelta(days=8)).date())
            ]
        else:
            curr = df_hist_b[df_hist_b["æ—¥æœŸ"] == (report_date - timedelta(days=1)).date()]
            prev = df_hist_b[df_hist_b["æ—¥æœŸ"] == (report_date - timedelta(days=8)).date()]

        curr_sum = summarize(curr, op_fields)
        prev_sum = summarize(prev, op_fields)
        link_ratio = {}
        for k in op_fields:
            if prev_sum.get(k,0):
                ratio = round(
                    (curr_sum.get(k,0) - prev_sum.get(k,0))
                    / (prev_sum.get(k,0) or 1)
                    * 100, 1
                )
                link_ratio[k] = f"{ratio}%"
            else:
                link_ratio[k] = "N/A"

        # 3) è®¡ç®—ä¸ 7 å¤©å‡å€¼ç¯æ¯”ï¼ˆå¯ç•™å­˜ä½†ä¸åœ¨æœ€ç»ˆè¾“å‡ºä¸­ï¼‰
        op7_sum = summarize(df_op7, op_fields)
        cmp7 = {
            k: f"{round((op_sum.get(k,0) - op7_sum.get(k,0)) /
                        (op7_sum.get(k,1) or 1) * 100, 1)}%"
            if op7_sum.get(k) else "N/A"
            for k in op_fields
        }

        # 4) è§£ææ¦œå•åŠ¨æ€
        non_null = df_op["rankings_detail"].dropna().tolist() if not df_op.empty else []
        raw_rank = non_null[0] if non_null else None
        if isinstance(raw_rank, (bytes, bytearray)):
            try: raw_rank = raw_rank.decode("utf-8")
            except: raw_rank = None
        if isinstance(raw_rank, str):
            try: parsed = json.loads(raw_rank)
            except: parsed = {}
        elif isinstance(raw_rank, dict):
            parsed = raw_rank
        else:
            parsed = {}
        if isinstance(parsed, str):
            try: tmp = json.loads(parsed)
            except: tmp = {}
            parsed = tmp if isinstance(tmp, dict) else {}
        rank_dict = parsed if isinstance(parsed, dict) else {}

        # åªä¿ç•™è¿™å‡ ä¸ªå¹³å°
        allowed = ["dianping_hot", "dianping_checkin", "dianping_rating"]
        # å±•ç¤ºé˜ˆå€¼
        th_city = 10
        th_sub = 5
        level_map = {"city": "å…¨å¸‚æ¦œ", "subdistrict": "åŒºå¿æ¦œ", "business": "å•†åœˆæ¦œ"}

        rank_lines = []
        for pf in allowed:
            scope = rank_dict.get(pf, {})
            if not isinstance(scope, dict):
                continue
            desc = RANK_PLATFORMS[pf]
            # åˆ¤æ–­å„çº§æ˜¯å¦è¾¾æ ‡
            city_rank = scope.get("city")
            sub_rank = scope.get("subdistrict")
            bus_rank = scope.get("business")
            # æ˜¯å¦å±•ç¤º city / subdistrict
            if isinstance(city_rank, int) and city_rank <= th_city:
                rank_lines.append(f"{desc}{level_map['city']}ç¬¬{city_rank}å")
            if isinstance(sub_rank, int) and sub_rank <= th_sub:
                rank_lines.append(f"{desc}{level_map['subdistrict']}ç¬¬{sub_rank}å")
            # å¦‚æœ city å’Œ subdistrict éƒ½ä¸è¾¾æ ‡ï¼Œåªå±•ç¤ºå•†åœˆæ¦œ
            if not (
                    (isinstance(city_rank, int) and city_rank <= th_city) or
                    (isinstance(sub_rank, int) and sub_rank <= th_sub)
            ) and isinstance(bus_rank, int):
                rank_lines.append(f"{desc}{level_map['business']}ç¬¬{bus_rank}å")

        if rank_lines:
            rank_note = "\n".join(rank_lines)
        else:
            rank_note = "æ— æ¦œå•å˜åŒ–"

        # 5) å–æ˜¨æ—¥å››é¡¹æ ¸å¿ƒæŒ‡æ ‡ & ç¯æ¯”
        today    = df_op.iloc[0]
        card_val = int(today['æ‰“å¡äººæ•°'])
        good_val = int(today['æ–°å¥½è¯„æ•°'])
        bad_val  = int(today['æ–°ä¸­å·®è¯„æ•°'])
        rev_val  = round(today['æ¶ˆè´¹é‡‘é¢'], 0)

        card_pct = link_ratio.get('æ‰“å¡äººæ•°',       'N/A')
        good_pct = link_ratio.get('æ–°å¥½è¯„æ•°',       'N/A')
        bad_pct  = link_ratio.get('æ–°ä¸­å·®è¯„æ•°',     'N/A')
        rev_pct  = link_ratio.get('æ¶ˆè´¹é‡‘é¢', 'N/A')


        # ç”¨ fmt() æ ¼å¼åŒ–ï¼Œå¹¶å¤„ç†â€œå·®è¯„ä¸º0â€åœºæ™¯
        card_str = fmt(card_val, card_pct)
        good_str = fmt(good_val, good_pct)
        bad_str  = "æš‚æ— å·®è¯„" if bad_val == 0 else fmt(bad_val, bad_pct)
        rev_str  = fmt(rev_val, rev_pct)

        # æ–°å¢æ”¶è—
        col_val = int(op_sum.get("æ–°å¢æ”¶è—äººæ•°", 0))
        col_pct = link_ratio.get("æ–°å¢æ”¶è—äººæ•°", "N/A")
        col_str = fmt(col_val, col_pct)

        # å¼‚åŠ¨æ£€æµ‹ï¼ˆæ›å…‰/è®¿é—® Â±30% åŠä»¥ä¸Šæ‰æç¤ºï¼‰
        extra_notes = []
        exp_pct = link_ratio.get("æ›å…‰äººæ•°", "N/A")
        if is_big_change(exp_pct):
            extra_notes.append(f"âš ï¸ æ›å…‰{exp_pct}")
        vis_pct = link_ratio.get("è®¿é—®äººæ•°", "N/A")
        if is_big_change(vis_pct):
            extra_notes.append(f"âš ï¸ è®¿é—®{vis_pct}")
        extra_note_str = "ï¼›" + "ï¼›".join(extra_notes) if extra_notes else ""

        # 6) æ‹¼è£…ä¸¤è¡Œè¾“å‡º
        suggestion = ("å‘ç°å·®è¯„ï¼Œè¿è¥å¸ˆå·²ä»‹å…¥è·Ÿè¿›ã€‚"
                      if bad_val > 0
                      else "æ•°æ®æ­£å¸¸ï¼Œç»§ç»­å¼•å¯¼å¥½è¯„ã€‚")
        # æ–°å¢æ”¶è—
        col_val = int(op_sum.get("æ–°å¢æ”¶è—äººæ•°", 0))
        col_pct = link_ratio.get("æ–°å¢æ”¶è—äººæ•°", "N/A")
        col_str = fmt(col_val, col_pct)

        # å¼‚åŠ¨æç¤º
        notes = []
        for k in ["æ›å…‰äººæ•°", "è®¿é—®äººæ•°"]:
            pct = link_ratio.get(k, "N/A")
            if is_big_change(pct):
                alias = "æ›å…‰" if k == "æ›å…‰äººæ•°" else "è®¿é—®"
                notes.append(f"{alias}{pct}")
        note_str = f" âš ï¸ {'/'.join(notes)}" if notes else ""

        # å·®è¯„
        bad_val = int(op_sum.get("æ–°ä¸­å·®è¯„æ•°", 0))
        bad_str = fmt(bad_val, link_ratio.get("æ–°ä¸­å·®è¯„æ•°", "N/A"))

        # å®šä¹‰TEXT
        text = (
            f"{brand}\n"
            f"- æ ¸å¿ƒæŒ‡æ ‡ï¼šæ¶ˆè´¹ {rev_str}ï¼›æ‰“å¡ {card_str}ï¼›æ”¶è— {col_str}ï¼›\n"
            f"  å¥½è¯„ {good_str}ï¼›{'' if bad_val == 0 else 'å·®è¯„ ' + bad_str}\n"
            f"- æ’è¡Œæ¦œï¼š\n{rank_note}\n"
            f"- å»ºè®®ï¼š{suggestion}\n"
        )

        # ä¿å­˜åˆ°å•åº— txtï¼ˆæ–‡ä»¶åï¼šå“ç‰Œ_æ—¥æœŸ.txtï¼‰
        #out_dir = Path("./daily_report")
        #out_dir.mkdir(exist_ok=True)
        #fn = out_dir / f"{brand}_{report_date.date()}.txt"
        #fn.write_text(text, encoding="utf-8-sig")
        #print(f"ğŸ“„ å·²ç”Ÿæˆï¼š{fn}")

        # æŒ‰ operator æ”¶é›†
        op = store_map.loc[store_map['brand_name'] == brand, 'operator'].iat[0]
        operator_sections[op].append(text)

        sections.append(
            f"{brand}\n"
            f"- æ ¸å¿ƒæŒ‡æ ‡ï¼šæ¶ˆè´¹ {rev_str}ï¼›æ‰“å¡ {card_str}ï¼›æ”¶è— {col_str}ï¼›\n"
            f"  å¥½è¯„ {good_str}ï¼›{'' if bad_val == 0 else 'å·®è¯„ ' + bad_str}\n"
            f"- æ’è¡Œæ¦œï¼š\n{rank_note}\n"
            f"- å»ºè®®ï¼š{'ğŸ‘ å¥½è¯„ç¨³å¢ï¼Œç»§ç»­å¼•å¯¼äº”æ˜Ÿ' if (good_val > 0 and bad_val == 0) else 'å·®è¯„å·²è½¬è¿è¥å¸ˆè·Ÿè¿›' if bad_val > 0 else 'æ•°æ®æ­£å¸¸'}\n"
        )

    # â€”â€” å¾ªç¯å¤–ï¼Œå†™å„è¿è¥å¸ˆçš„ TXT â€”â€”
    for op, texts in operator_sections.items():
        fn = out_dir / f"{op}_{report_date.date()}.txt"
        fn.write_text("\n\n".join(texts), encoding="utf-8-sig")
        print(f"ğŸ“„ å·²ç”Ÿæˆè¿è¥å¸ˆæ—¥æŠ¥ï¼š{fn}")

    # â€”â€” ä¸€æ¬¡æ€§æ‹‰å½“æœˆå…¨é‡æ•°æ® & è¡ç”Ÿåˆ—ï¼ˆå¾ªç¯å¤–ï¼‰ â€”â€”
    month_start = report_date.replace(day=1).date()
    df_month = pd.read_sql(
        f"""
        SELECT
          `ç¾å›¢é—¨åº—ID`,`æ—¥æœŸ`,`æ¶ˆè´¹é‡‘é¢`,`æ›å…‰äººæ•°`,`è®¿é—®äººæ•°`,`è´­ä¹°äººæ•°`,
          `æ‰«ç äººæ•°`,`æ–°å¢æ”¶è—äººæ•°`,`æ‰“å¡äººæ•°`,`æ–°å¥½è¯„æ•°`,`æ–°ä¸­å·®è¯„æ•°`,`ç‚¹è¯„æ˜Ÿçº§`
        FROM `operation_data`
        WHERE `æ—¥æœŸ` BETWEEN '{month_start}' AND '{report_date.date()}'
        """, engine
    ).merge(
        store_map[['ç¾å›¢é—¨åº—ID','brand_name','operator']],
        on='ç¾å›¢é—¨åº—ID', how='left'
    )
    # è¡ç”Ÿâ€œæ˜ŸæœŸâ€ã€â€œè®¿é—®è½¬åŒ–â€ã€â€œè´­ä¹°è½¬åŒ–â€
    weekday_map = {0:'æ˜ŸæœŸä¸€',1:'æ˜ŸæœŸäºŒ',2:'æ˜ŸæœŸä¸‰',3:'æ˜ŸæœŸå››',4:'æ˜ŸæœŸäº”',5:'æ˜ŸæœŸå…­',6:'æ˜ŸæœŸæ—¥'}
    df_month['æ˜ŸæœŸ'] = pd.to_datetime(df_month['æ—¥æœŸ']).dt.weekday.map(weekday_map)
    df_month['è®¿é—®è½¬åŒ–'] = (df_month['è®¿é—®äººæ•°']/df_month['æ›å…‰äººæ•°']).round(3)
    df_month['è´­ä¹°è½¬åŒ–'] = (df_month['è´­ä¹°äººæ•°']/df_month['è®¿é—®äººæ•°']).round(3)
    cols = ['æ—¥æœŸ','æ˜ŸæœŸ','æ¶ˆè´¹é‡‘é¢','æ›å…‰äººæ•°','è®¿é—®äººæ•°','è´­ä¹°äººæ•°',
            'è®¿é—®è½¬åŒ–','è´­ä¹°è½¬åŒ–','æ–°å¢æ”¶è—äººæ•°','æ‰“å¡äººæ•°',
            'æ–°å¥½è¯„æ•°','æ–°ä¸­å·®è¯„æ•°','æ‰«ç äººæ•°','ç‚¹è¯„æ˜Ÿçº§']

    # â€”â€” æŒ‰è¿è¥å¸ˆè¾“å‡ºæœˆåº¦ Excelï¼ˆæ¯åº—ä¸€ä¸ª sheetï¼‰ â€”â€”
    for op, grp_op in df_month.groupby('operator'):
        file = out_dir / f"{op}_{report_date.date()}_æœˆåº¦æ•°æ®.xlsx"
        # 1) å†™æ¯ä¸ªé—¨åº—åˆ†åˆ«ä¸€ä¸ª sheet
        with pd.ExcelWriter(file, engine='openpyxl', mode='w') as writer:
            for brand, grp in grp_op.groupby('brand_name'):
                sheet = grp.sort_values('æ—¥æœŸ')[cols]
                sheet.to_excel(
                    writer,
                    sheet_name=brand[:31],  # sheet åç§°æˆª 31 å­—
                    index=False,
                    startrow=2
                )
        print(f"âœ… å·²ç”Ÿæˆè¿è¥å¸ˆæœˆåº¦ Excelï¼š{file}")

        # 4) æ ·å¼ + å†»ç»“ + æ ¼å¼åŒ– â€”â€” è¿™é‡Œç”¨ file è€Œé out_excel
        wb = openpyxl.load_workbook(file)
        header_fill = PatternFill("solid", fgColor="DDDDDD")
        for ws in wb.worksheets:
            max_col = ws.max_column

            # â€”â€”â€” åˆå¹¶ç¬¬1è¡Œå†™é—¨åº—åç§° â€”â€”â€”
            ws.merge_cells(start_row=1, start_column=1, end_row=1, end_column=max_col)
            title = ws.cell(row=1, column=1)
            title.value = ws.title
            title.font = Font(size=14, bold=True)
            title.alignment = Alignment(horizontal="center", vertical="center")

            # â€”â€”â€” ç¬¬2è¡Œç•™ç™½ â€”â€”â€”
            # ï¼ˆä»€ä¹ˆéƒ½ä¸ç”¨å†™ï¼Œè‡ªåŠ¨ä¿ç•™ç©ºè¡Œï¼‰

            # â€”â€”â€” ç¬¬3è¡Œæ ·å¼ï¼šåŠ ç²—å±…ä¸­ï¼‹ç°åº• â€”â€”â€”
            for col_idx in range(1, max_col + 1):
                cell = ws.cell(row=3, column=col_idx)
                cell.font = Font(bold=True)
                cell.fill = header_fill
                cell.alignment = Alignment(horizontal="center", vertical="center")

            # â€”â€”â€” å†»ç»“å‰ä¸‰è¡Œ â€”â€”â€”
            ws.freeze_panes = "A4"

            # â€”â€”â€” åˆ—å®½è‡ªé€‚åº” & å¼ºåˆ¶æœ€å°å®½åº¦ â€”â€”â€”
            # ä¸ºäº†è®© A åˆ—ï¼ˆæ—¥æœŸï¼‰ä¸è¦è¿‡å®½ï¼ŒD~L åˆ—è¶³å¤Ÿå®½ï¼Œå®šä¹‰æœ€å°å®½åº¦ï¼š
            min_widths = {
                'A': 13,  # æ—¥æœŸ
                'B': 8,  # æ˜ŸæœŸ
                'C': 10,  # æ¶ˆè´¹é‡‘é¢
                'D': 10, 'E': 10, 'F': 10, 'G': 10,
                'H': 10, 'I': 13, 'J': 10, 'K': 10, 'L': 11,
                'M': 8,  # æ‰«ç 
                'N': 8  # æ˜Ÿçº§
            }
            for col_cells in ws.columns:
                col_letter = get_column_letter(col_cells[0].column)
                # å¦‚æœæ˜¯æ—¥æœŸåˆ—ï¼Œç›´æ¥ç”¨å›ºå®šå®½åº¦
                if col_letter == 'A':
                    ws.column_dimensions[col_letter].width = min_widths['A']
                    continue

                # å…¶å®ƒåˆ—ç…§å¸¸è‡ªé€‚åº”ï¼Œä½†ä¿è¯ä¸å°äº min_widths
                max_len = max(
                    len(str(c.value)) if c.value is not None else 0
                    for c in col_cells
                )
                optimal = max_len + 4
                ws.column_dimensions[col_letter].width = max(
                    optimal, min_widths.get(col_letter, optimal)
                )

            # â€”â€”â€” å•å…ƒæ ¼æ ¼å¼åŒ– â€”â€”â€”
            money_fmt = '#,##0'
            pct_fmt = '0.0%'
            star_fmt = '0.0'
            # â€œç‚¹è¯„æ˜Ÿçº§â€åœ¨æœ€åä¸€åˆ—ï¼Œè¿™é‡Œç”¨ max_column ä¿è¯å®šä½
            for row in range(4, ws.max_row + 1):
                ws.cell(row, 3).number_format = money_fmt  # C åˆ—ï¼šæ¶ˆè´¹é‡‘é¢
                ws.cell(row, 7).number_format = pct_fmt  # G åˆ—ï¼šè®¿é—®è½¬åŒ–
                ws.cell(row, 8).number_format = pct_fmt  # H åˆ—ï¼šè´­ä¹°è½¬åŒ–
                ws.cell(row, ws.max_column).number_format = star_fmt  # N åˆ—ï¼šæ˜Ÿçº§

            # â€”â€”â€” å•å…ƒæ ¼æ ¼å¼åŒ– â€”â€”â€”
            money_fmt = '#,##0'
            pct_fmt = '0.0%'
            star_fmt = '0.0'
            # å‡è®¾åˆ—åºï¼šC=æ¶ˆè´¹é‡‘é¢, G=è®¿é—®è½¬åŒ–, H=è´­ä¹°è½¬åŒ–, N=ç‚¹è¯„æ˜Ÿçº§
            ws.column_dimensions['C'].width += 2
            for row in range(4, ws.max_row + 1):
                ws.cell(row, 3).number_format = money_fmt
                ws.cell(row, 7).number_format = pct_fmt
                ws.cell(row, 8).number_format = pct_fmt
                ws.cell(row, ws.max_column).number_format = star_fmt

        wb.save(file)
        print(f"âœ… Excel å·²æ ¼å¼åŒ–ï¼š{file}")

    # ---------- æ¯ 5 å®¶ä¸€ç»„è°ƒç”¨ AI è¾“å‡º ----------
    '''md_chunks = []
    group_size = 5
    header = (
        "ä½ æ˜¯é¤é¥®BIè¿è¥åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹å“ç‰Œæ˜¨æ—¥å…³é”®æ•°æ®ï¼Œ"
        "ç”¨ Markdown è¾“å‡ºï¼Œæ¯å®¶åº—ä»…ä¸¤è¡Œï¼š\n"
        "ç¬¬ä¸€è¡Œï¼šæ˜¨æ—¥æ‰“å¡äººæ•°ã€å¥½è¯„æ•°ã€ä¸­å·®è¯„æ•°ã€æ¶ˆè´¹é‡‘é¢ç¯æ¯”ï¼›\n"
        "ç¬¬äºŒè¡Œï¼šç»™å‡ºæ¦œå•ä¿¡æ¯ï¼Œå¹¶é™„ä¸Šâ€œè¯·åº—å†…åŒäº‹â€¦â€¦â€å¼çš„è¡ŒåŠ¨å»ºè®®ï¼›\n"
        "å…¶ä½™ä¸å¿…èµ˜è¿°ï¼Œå…¨éƒ¨ç”¨ä¸­æ–‡ã€‚\n\n"
    )

    for idx in range(0, len(sections), group_size):
        batch    = sections[idx:idx+group_size]
        prompt   = header + "\n\n".join(batch)
        batch_no = idx // group_size + 1
        print(f"â¡ï¸ è°ƒç”¨ AIï¼ˆç¬¬{batch_no}æ‰¹ï¼‰")
        for attempt in range(1, 4):
            try:
                md = call_kimi_api([{"role":"user","content":prompt}], API_KEY, MODEL, temperature=0.3)
                md_chunks.append(md)
                break
            except SSLError:
                print(f"âš ï¸ AI è°ƒç”¨ç¬¬{batch_no}æ‰¹ç¬¬{attempt}æ¬¡å¤±è´¥ï¼Œé‡è¯•â€¦")
                time.sleep(attempt * 2)
        else:
            print(f"âŒ ç¬¬{batch_no}æ‰¹æœ€ç»ˆå¤±è´¥ï¼Œè·³è¿‡è¯¥æ‰¹å†…å®¹")
            md_chunks.append(f"### ç¬¬{batch_no}æ‰¹å†…å®¹ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åæŸ¥çœ‹ã€‚")

    # åˆå¹¶å¹¶å†™æ–‡ä»¶
    md_all = "\n\n---\n\n".join(md_chunks)
    out_dir = Path("./daily_report")
    out_dir.mkdir(exist_ok=True)
    fp = out_dir / f"{report_date.date()}.md"
    fp.write_text(md_all, encoding="utf-8")
    print("âœ… æ—¥æŠ¥ç”Ÿæˆï¼š", fp)'''

if __name__ == "__main__":
    main()
