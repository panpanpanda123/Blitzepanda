# main.pyï¼ˆè‡ªåŠ¨è¯»å–å›ºå®šè·¯å¾„ + å»é‡å¤„ç†ï¼‰
import os
import pandas as pd
import json, math, re
from sqlalchemy import create_engine, text, inspect
from column_mappings import COLUMN_MAPPING_CPC_HOURLY
from excel_header_finder import clean_and_load_excel
from data_cleaning import (
    clean_operation_data,
    clean_numeric_columns,
    drop_percentage_columns,
    match_store_id_for_single_cpc,
    process_cpc_dates,add_datetime_column,
)
from database_importer import import_to_mysql, get_dtype_for_cpc_hourly
from config import DB_CONNECTION_STRING
import shutil
from datetime import datetime
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module='openpyxl')
import logging
logging.basicConfig(level=logging.INFO,
                    format='%(levelname)s: %(message)s')

# å¼•å…¥ç‚¹è¯„æ¸…æ´—å‡½æ•° & MySQL å†™å…¥
from review_cleaner import clean_review_file
from sqlalchemy import types as sqltypes

engine = create_engine(DB_CONNECTION_STRING)

def build_rankings_detail(row):
    """
    ç›´æ¥ç”¨å½“å‰è¡Œçš„ 'åŸå¸‚' åˆ—åˆ¤å®š city çº§ï¼Œ
    ä»¥ 'åŒº' ç»“å°¾åˆ¤å®š district çº§ï¼Œ
    å…¶ä½™å½“ subdistrict çº§ã€‚
    """
    city = str(row.get("åŸå¸‚", "")).strip()
    cols = {
        "ç¾å›¢äººæ°”æ¦œæ¦œå•æ’å":   "meituan_popularity",
        "ç¾å›¢å¥½è¯„æ¦œæ¦œå•æ’å":   "meituan_rating",
        "ç‚¹è¯„çƒ­é—¨æ¦œæ¦œå•æ’å":   "dianping_hot",
        "ç‚¹è¯„å¥½è¯„æ¦œæ¦œå•æ’å":   "dianping_rating",
        "ç‚¹è¯„å£å‘³æ¦œæ¦œå•æ’å":   "dianping_taste",
        "ç‚¹è¯„ç¯å¢ƒæ¦œæ¦œå•æ’å":   "dianping_env",
        "ç‚¹è¯„æœåŠ¡æ¦œæ¦œå•æ’å":   "dianping_service",
        "ç‚¹è¯„æ‰“å¡äººæ°”æ¦œæ¦œå•æ’å": "dianping_checkin",
    }
    detail = {}
    for src_col, key in cols.items():
        raw = row.get(src_col)
        if not raw or pd.isna(raw):
            continue
        tmp = {}
        for part in str(raw).split("|"):
            m = re.search(r'(.+?)ç¬¬(\d+)å', part.strip())
            if not m:
                continue
            scope = m.group(1).strip()
            rank  = int(m.group(2))
            # åˆ¤å®šå±‚çº§
            if city and city in scope:
                tmp['city'] = rank
            elif scope.endswith('åŒº'):
                tmp['district'] = rank
            else:
                tmp['subdistrict'] = rank
        if tmp:
            detail[key] = tmp
    return json.dumps(detail, ensure_ascii=False)
# â€”â€” END ç®€åŒ–ç‰ˆ build_rankings_detail â€”â€”

def bulk_import_excels_to_table(folder_path: str, table_name: str):
    """
    æ‰«æ folder_path ä¸‹æ‰€æœ‰ .xlsxï¼ŒæŠŠæ¯ä¸ªæ–‡ä»¶è¯»æˆ DataFrameï¼Œ
    è‡ªåŠ¨å¯¹é½ table_name é‡Œå·²æœ‰çš„åˆ—ï¼Œå‰©ä¸‹çš„ä¸¢å¼ƒï¼Œ
    ç„¶å append åˆ°è¯¥è¡¨ä¸­ã€‚
    """
    engine = create_engine(DB_CONNECTION_STRING)
    inspector = inspect(engine)
    # 1. åå°„è¡¨ç»“æ„ï¼Œæ‹¿åˆ°è¡¨é‡Œæ‰€æœ‰åˆ—å
    existing_cols = [c['name'] for c in inspector.get_columns(table_name)]

    for fname in os.listdir(folder_path):
        if not fname.endswith(".xlsx"):
            continue
        full_path = os.path.join(folder_path, fname)
        # 2. è¯» Excelï¼ˆé»˜è®¤ç¬¬ä¸€å¼ è¡¨ï¼‰
        df = pd.read_excel(full_path)
        # 3. é‡å‘½åæ˜ å°„ï¼ˆå¦‚éœ€ï¼‰ï¼Œå¦åˆ™æ³¨é‡Šæ‰ä¸‹ä¸€è¡Œ
        # df = df.rename(columns=COLUMN_MAPPING_OPERATION)
        # 4. åªä¿ç•™è¡¨é‡Œå·²æœ‰çš„åˆ—
        df = df[[c for c in df.columns if c in existing_cols]]
        if df.empty:
            continue
        # 5. å†™å…¥ MySQL
        df.to_sql(table_name, engine, if_exists="append", index=False)
        print(f"âœ… {fname} å¯¼å…¥è¡¨ {table_name} å®Œæˆï¼Œ{len(df)} è¡Œã€‚")

# âœ… å›ºå®šæ•°æ®è·¯å¾„ï¼ˆè‡ªåŠ¨è¯»å–ï¼‰
FIXED_FOLDER_PATH = r"D:\æ©¡çš®ä¿¡æ¯ç§‘æŠ€\æ©¡çš®å®¢æˆ·è¿è¥\å¤§ä¼—ç‚¹è¯„è¿è¥æ•°æ®\raw_data"

# 1) å®šä¹‰æ¸…ç†å‡½æ•°ï¼Œæ›¿ä»£æ—§ delete_data_by_date.py
def delete_old_cpc_for_store(start_date, end_date, store_ids):
    """åªåˆ é™¤æŒ‡å®šé—¨åº—åœ¨æ—¥æœŸèŒƒå›´å†…çš„æ¨å¹¿é€šï¼ˆcpc_hourly_dataï¼‰æ—§æ•°æ®"""
    engine = create_engine(DB_CONNECTION_STRING)
    with engine.begin() as conn:
        for sid in store_ids:
            res = conn.execute(
                text("DELETE FROM cpc_hourly_data WHERE store_id = :sid AND DATE(date) BETWEEN :s AND :e"),
                {"sid": sid, "s": start_date, "e": end_date}
            )
            print(f"âœ… åˆ é™¤ cpc_hourly_data ä¸­ é—¨åº—ID={sid} {start_date}~{end_date} å…± {res.rowcount} æ¡ã€‚")

def delete_old_op_for_store(start_date, end_date, store_ids):
    """åªåˆ é™¤æŒ‡å®šé—¨åº—åœ¨æ—¥æœŸèŒƒå›´å†…çš„è¿è¥æ•°æ®ï¼ˆoperation_dataï¼‰æ—§è®°å½•"""
    engine = create_engine(DB_CONNECTION_STRING)
    with engine.begin() as conn:
        for sid in store_ids:
            res = conn.execute(
                text("DELETE FROM operation_data WHERE `ç¾å›¢é—¨åº—ID` = :sid AND DATE(`æ—¥æœŸ`) BETWEEN :s AND :e"),
                {"sid": sid, "s": start_date, "e": end_date}
            )
            print(f"âœ… åˆ é™¤ operation_data ä¸­ é—¨åº—ID={sid} {start_date}~{end_date} å…± {res.rowcount} æ¡ã€‚")


# 2) å®šä¹‰å·²å¤„ç†ç›®å½•å¸¸é‡
PROCESSED_ROOT_PATH = r"D:\æ©¡çš®ä¿¡æ¯ç§‘æŠ€\æ©¡çš®å®¢æˆ·è¿è¥\å¤§ä¼—ç‚¹è¯„è¿è¥æ•°æ®\processed_data"

def process_brand(brand_path, store_mapping):
    """
    è¯»å– brand_path ä¸‹æ‰€æœ‰æ¨å¹¿æŠ¥è¡¨æ–‡ä»¶ï¼Œæ¸…æ´—ã€åˆå¹¶å¹¶è¿”å› DataFrame
    """
    hourly_cpc = []
    for fname in os.listdir(brand_path):
        if not (fname.endswith(".xlsx") and "æ¨å¹¿æŠ¥è¡¨" in fname):
            continue
        fp = os.path.join(brand_path, fname)
        df = clean_and_load_excel(fp)
        df = process_cpc_dates(df, fname)
        df = match_store_id_for_single_cpc(df, store_mapping)
        df = drop_percentage_columns(df)
        df = clean_numeric_columns(df)
        df = add_datetime_column(df)
        hourly_cpc.append(df)
    if not hourly_cpc:
        return pd.DataFrame()  # æ²¡æœ‰æŠ¥è¡¨åˆ™è¿”å›ç©º DF
    df_all = pd.concat(hourly_cpc, ignore_index=True)
    # ç”Ÿæˆå”¯ä¸€æ ‡è¯† plan_key
    df_all['plan_key'] = (
        df_all['é—¨åº—ID'].astype(str).str.strip()
        + "_"
        + df_all['æ¨å¹¿åç§°'].astype(str).str.strip()
        + "_"
        + df_all['å¹³å°'].astype(str).str.strip()
    )
    # é‡å‘½åä¸ºè‹±æ–‡åˆ—
    df_all.rename(columns=COLUMN_MAPPING_CPC_HOURLY, inplace=True)
    # å»é‡ï¼šåŒä¸€ä¸ª plan_key + åŒä¸€ä¸ªæ—¶æ®µ åªä¿ç•™æœ€åä¸€æ¡
    df_all.drop_duplicates(subset=['plan_key', 'start_time'], inplace=True)
    return df_all

def process_files():
    base = FIXED_FOLDER_PATH
    print(f"ğŸ“‚ æ­£åœ¨è¯»å–å›ºå®šè·¯å¾„ï¼š{base}")

    engine = create_engine(DB_CONNECTION_STRING)
    store_mapping = pd.read_sql("SELECT * FROM store_mapping", con=engine)
    store_mapping["æ¨å¹¿é—¨åº—"] = store_mapping["æ¨å¹¿é—¨åº—"].str.strip()
    store_mapping["é—¨åº—ID"] = store_mapping["é—¨åº—ID"].astype(str).str.strip()

    cpc_successes, op_successes, failures = [], [], []

    for brand in os.listdir(base):
        brand_dir = os.path.join(base, brand)
        if not os.path.isdir(brand_dir):
            continue

        try:
            # â€”â€” 1) å¤„ç†æ¨å¹¿é€š â€”â€”
            # åªå¤„ç†åŒ…å«â€œæ¨å¹¿æŠ¥è¡¨â€ã€â€œè´¦å·æŠ¥è¡¨â€æˆ–â€œé—¨åº—æŠ¥è¡¨â€çš„æ–‡ä»¶ï¼Œé¿å…å°†å…¶ä»– Excel å½“ä½œ CPC å¯¼å…¥
            cpc_files = [
                f for f in os.listdir(brand_dir)
                if f.endswith(".xlsx") and any(k in f for k in ["æ¨å¹¿æŠ¥è¡¨", "è´¦å·æŠ¥è¡¨", "é—¨åº—æŠ¥è¡¨"])
            ]
            if cpc_files:
                hourly_list = []
                for fname in cpc_files:
                    fp = os.path.join(brand_dir, fname)
                    df = clean_and_load_excel(fp)
                    df = process_cpc_dates(df, fname)
                    df = match_store_id_for_single_cpc(df, store_mapping)
                    df = drop_percentage_columns(df)
                    df = clean_numeric_columns(df)
                    df = add_datetime_column(df)
                    hourly_list.append(df)
                df_cpc = pd.concat(hourly_list, ignore_index=True)

                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†å¹¶é‡å‘½åã€å»é‡
                df_cpc['plan_key'] = (
                    df_cpc['é—¨åº—ID'].astype(str).str.strip()
                    + "_"
                    + df_cpc['æ¨å¹¿åç§°'].astype(str).str.strip()
                    + "_"
                    + df_cpc['å¹³å°'].astype(str).str.strip()
                )
                df_cpc.rename(columns=COLUMN_MAPPING_CPC_HOURLY, inplace=True)
                df_cpc.drop_duplicates(subset=['plan_key', 'start_time'], inplace=True)

                # åˆ é™¤å†å²åŒåº—åŒæ—¥æœŸæ•°æ®
                df_cpc['date'] = pd.to_datetime(df_cpc['date']).dt.date
                min_date, max_date = df_cpc['date'].min(), df_cpc['date'].max()
                store_ids_cpc = df_cpc['store_id'].astype(str).unique().tolist()
                delete_old_cpc_for_store(min_date, max_date, store_ids_cpc)

                # å†™å…¥æ–°æ•°æ®
                dtype_cpc = get_dtype_for_cpc_hourly(df_cpc)
                import_to_mysql(
                    df_cpc,
                    "cpc_hourly_data",
                    DB_CONNECTION_STRING,
                    dtype=dtype_cpc,
                    if_exists="append"
                )
                cpc_successes.append(brand)
            else:
                logging.info(f"å“ç‰Œ {brand} ä¸‹æ—  CPC ç›¸å…³æŠ¥è¡¨ï¼Œè·³è¿‡ã€‚")

            # â€”â€” 2) å¤„ç†è¿è¥æ•°æ® â€”â€”
            op_dfs = []
            for fname in os.listdir(brand_dir):
                # åªå¤„ç†è¿è¥è¡¨ï¼Œè·³è¿‡æ‰€æœ‰å¸¦â€œæ¨å¹¿æŠ¥è¡¨â€æˆ–â€œè¯„ä»·â€å…³é”®å­—çš„ .xlsx
                if (not fname.endswith(".xlsx")
                        or "æ¨å¹¿æŠ¥è¡¨" in fname
                        or "è¯„ä»·" in fname
                ):
                    continue
                fp = os.path.join(brand_dir, fname)
                df_op = clean_operation_data(clean_and_load_excel(fp))
                df_op = drop_percentage_columns(df_op)
                df_op = clean_numeric_columns(df_op)
                op_dfs.append(df_op)

            if op_dfs:
                # åˆå¹¶å»é‡
                df_op_all = pd.concat(op_dfs, ignore_index=True)
                df_op_all.drop_duplicates(subset=["æ—¥æœŸ", "ç¾å›¢é—¨åº—ID"], keep="last", inplace=True)

                # è½¬ datetime å¹¶å–åŒºé—´
                df_op_all["æ—¥æœŸ"] = pd.to_datetime(df_op_all["æ—¥æœŸ"]).dt.date
                min_op, max_op = df_op_all["æ—¥æœŸ"].min(), df_op_all["æ—¥æœŸ"].max()
                store_ids_op = df_op_all["ç¾å›¢é—¨åº—ID"].astype(str).unique().tolist()

                # åˆ é™¤æ—§è®°å½•
                delete_old_op_for_store(min_op, max_op, store_ids_op)

                # åå°„è¡¨ç»“æ„ï¼Œå–å·²å­˜åœ¨çš„åˆ—ä½œä¸º flat åˆ—
                inspector     = inspect(engine)
                existing_cols = [c["name"] for c in inspector.get_columns("operation_data")]
                flat_cols     = [c for c in existing_cols if c in df_op_all.columns]
                df_basic      = df_op_all[flat_cols].copy()

                # â€”â€” dynamic extra_metrics â€”â€”
                dynamic_df = df_op_all.drop(columns=flat_cols, errors="ignore")
                dicts = dynamic_df.to_dict(orient="records")
                if not dicts:
                    dicts = [{} for _ in range(len(df_basic))]
                cleaned = []
                for d in dicts:
                    cleaned.append({
                        k: (None if isinstance(v, float) and math.isnan(v) else v)
                        for k, v in d.items()
                    })
                df_basic["extra_metrics"] = [json.dumps(d, ensure_ascii=False) for d in cleaned]

                # â€”â€” ROS åˆ† â€”â€”
                if "ROSåˆ†" in df_op_all.columns:
                    df_basic["ros_score"] = pd.to_numeric(
                        df_op_all["ROSåˆ†"], errors="coerce"
                    ).fillna(0).astype(int)

                # â€”â€” æ’è¡Œæ¦œè¯¦æƒ… JSON â€”â€”
                df_basic["rankings_detail"] = df_op_all.apply(build_rankings_detail, axis=1)

                # â€”â€” å”¯ä¸€ä¸€æ¬¡å†™å…¥ operation_data â€”â€”
                from database_importer import get_dtype_for_operation
                dtype_op = get_dtype_for_operation(df_basic)
                import_to_mysql(
                    df_basic,
                    "operation_data",
                    DB_CONNECTION_STRING,
                    dtype=dtype_op,
                    if_exists="append"
                )
                op_successes.append(brand)
            else:
                logging.info(f"å“ç‰Œ {brand} ä¸‹æ— è¿è¥æ•°æ®ï¼Œè·³è¿‡ã€‚")

            # â€”â€” 3) å¤„ç†è¯„ä»·æ•°æ® â€”â€”  <â€” ä¸ä¸Šé¢ if/else åŒçº§
            review_files = [
                f for f in os.listdir(brand_dir)
                if f.endswith(".xlsx") and "è¯„ä»·" in f
            ]
            if review_files:
                for fname in review_files:
                    fp = os.path.join(brand_dir, fname)
                    df_rev = clean_review_file(fp, store_mapping)
                    dtype_review = {
                        "store_id": sqltypes.String(50),
                        "review_date": sqltypes.Date,
                        "rating_raw": sqltypes.Numeric(3, 1),
                        "rating_label": sqltypes.String(2),
                        "review_text": sqltypes.Text,
                        "senti_score": sqltypes.Float,
                        "senti_label": sqltypes.String(2),
                        "key_topics": sqltypes.JSON,
                    }
                    import_to_mysql(
                        df_rev,
                        "review_data",
                        DB_CONNECTION_STRING,
                        dtype=dtype_review,
                        if_exists="append"
                    )
                    print(f"âœ… {brand} çš„è¯„ä»·æ–‡ä»¶ {fname} å·²å†™å…¥ review_dataï¼Œå…± {len(df_rev)} è¡Œã€‚")
            else:
                logging.info(f"å“ç‰Œ {brand} ä¸‹æ— è¯„ä»·æ–‡ä»¶ï¼Œè·³è¿‡ã€‚")
            # â€”â€” 4) å…¨éƒ¨æˆåŠŸåæ¬ç›®å½• â€”â€”
            move_processed_files(brand_dir, PROCESSED_ROOT_PATH)

        except Exception as e:
            logging.error(f"å“ç‰Œ {brand} å¤„ç†å¤±è´¥ï¼š{e}")
            failures.append((brand, str(e)))

    # æœ€ç»ˆç»“æœ
    print(f"âœ… æ¨å¹¿é€šæˆåŠŸå“ç‰Œï¼š{cpc_successes}")
    print(f"âœ… è¿è¥æ•°æ®æˆåŠŸå“ç‰Œï¼š{op_successes}")
    if failures:
        print("âŒ ä»¥ä¸‹å“ç‰Œå¤„ç†å¤±è´¥ï¼Œè¯·äººå·¥ä»‹å…¥ï¼š")
        for b, err in failures:
            print(f"  - {b}: {err}")



def move_processed_files(src_root, dst_root):
    for root, dirs, files in os.walk(src_root):
        for file in files:
            if file.startswith("~$") or not file.endswith(".xlsx"):
                continue
            src_file_path = os.path.join(root, file)

            # è®¡ç®—ç›¸å¯¹å­è·¯å¾„
            relative_subdir = os.path.relpath(root, src_root)
            dst_subdir_path = os.path.join(dst_root, relative_subdir)

            os.makedirs(dst_subdir_path, exist_ok=True)  # è‡ªåŠ¨åˆ›å»ºç›®æ ‡å­ç›®å½•

            dst_file_path = os.path.join(dst_subdir_path, file)
            shutil.move(src_file_path, dst_file_path)
            print(f"ğŸ“¦ å·²ç§»åŠ¨æ–‡ä»¶åˆ°ï¼š{dst_file_path}")

if __name__ == "__main__":
    process_files()
