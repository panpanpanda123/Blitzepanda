"""
app.pipelines

æŒ‰ç›®å½•æ‰¹é‡å¯¼å…¥â€œè¿è¥æ•°æ® / å°æ—¶çº§ CPC æ•°æ®â€çš„è½»é‡æµæ°´çº¿ã€‚
ä¸æ—§è„šæœ¬çš„å…¥åº“é€»è¾‘å¯¹é½ï¼Œä½†æŠ½ç¦»æˆå¯å¤ç”¨å‡½æ•°ã€‚
"""

from __future__ import annotations

import os
import json
from typing import Tuple

import pandas as pd
from sqlalchemy import text

from app.db import get_engine, import_to_mysql, reflect_existing_columns, get_dtype_for_operation, get_dtype_for_cpc_hourly
from app.excel import clean_and_load_excel
from app.cleaning import (
    clean_operation_data, drop_percentage_columns, clean_numeric_columns,
    match_store_id_for_single_cpc, process_cpc_dates, add_datetime_column,
)
from app.mappings import COLUMN_MAPPING_CPC_HOURLY


def _build_rankings_detail(row: pd.Series) -> str:
    # ä¸æ—§å®ç°ä¿æŒä¸€è‡´çš„ç®€åŒ–ç‰ˆ
    import re
    city = str(row.get("åŸå¸‚", "")).strip()
    cols = {
        "ç¾å›¢äººæ°”æ¦œæ¦œå•æ’å":   "meituan_popularity",
        "ç¾å›¢å¥½è¯„æ¦œæ¦œå•æ’å":   "meituan_rating",
        "ç‚¹è¯„çƒ­é—¨æ¦œæ¦œå•æ’å":   "dianping_hot",
        "ç‚¹è¯„å¥½è¯„æ¦œæ¦œå•æ’å":   "dianping_rating",
        "ç‚¹è¯„å£å‘³æ¦œæ¦œå•æ’å":   "dianping_taste",
        "ç‚¹è¯„ç¯å¢ƒæ¦œæ¦œå•æ’å":   "dianping_env",
        "ç‚¹è¯„æœåŠ¡æ¦œæ¦œå•æ’å":   "dianping_service",
        "ç‚¹è¯„æ‰“å¡äººæ°”æ¦œæ¦œå•æ’å": "dianping_checkin",
    }
    detail = {}
    for src_col, key in cols.items():
        raw = row.get(src_col)
        if raw is None or (isinstance(raw, float) and pd.isna(raw)):
            continue
        tmp = {}
        for part in str(raw).split("|"):
            m = re.search(r"(.+?)ç¬¬(\d+)å", part.strip())
            if not m:
                continue
            scope = m.group(1).strip()
            rank = int(m.group(2))
            if city and city in scope:
                tmp["city"] = rank
            elif scope.endswith("åŒº"):
                tmp["district"] = rank
            else:
                tmp["subdistrict"] = rank
        if tmp:
            detail[key] = tmp
    return json.dumps(detail, ensure_ascii=False)


def import_operation_folder(folder: str) -> None:
    print(f"ğŸ“‚ æ­£åœ¨è¯»å–è¿è¥æ•°æ®è·¯å¾„ï¼š{folder}")
    os.makedirs(folder, exist_ok=True)
    dfs = []
    for fname in os.listdir(folder):
        if not fname.endswith(".xlsx"):
            continue
        fp = os.path.join(folder, fname)
        try:
            df = clean_operation_data(clean_and_load_excel(fp))
            df = drop_percentage_columns(df)
            df = clean_numeric_columns(df, key_col="ç¾å›¢é—¨åº—ID")
            dfs.append(df)
        except Exception as e:
            print(f"âŒ è¿è¥æ•°æ®æ–‡ä»¶ {fp} å¤„ç†å¤±è´¥: {e}")
    if not dfs:
        print("âš ï¸ æ— è¿è¥æ•°æ®æ–‡ä»¶å¯¼å…¥"); return

    df_all = pd.concat(dfs, ignore_index=True)
    # å…œåº•ï¼šè‹¥ä»ä¸å­˜åœ¨å…³é”®åˆ—ï¼Œæ‰“å°åˆ—åå¹¶æŠ¥é”™æç¤º
    required_cols = ["æ—¥æœŸ", "ç¾å›¢é—¨åº—ID"]
    missing = [c for c in required_cols if c not in df_all.columns]
    if missing:
        print(f"âŒ è¿è¥æ•°æ®ç¼ºå°‘å…³é”®åˆ—ï¼š{missing}ã€‚å½“å‰åˆ—ï¼š{list(df_all.columns)}")
        raise KeyError(missing)
    df_all.drop_duplicates(subset=required_cols, keep="last", inplace=True)
    df_all["æ—¥æœŸ"] = pd.to_datetime(df_all["æ—¥æœŸ"]).dt.date
    min_date, max_date = df_all["æ—¥æœŸ"].min(), df_all["æ—¥æœŸ"].max()
    store_ids = df_all["ç¾å›¢é—¨åº—ID"].astype(str).unique().tolist()

    engine = get_engine()
    with engine.begin() as conn:
        for sid in store_ids:
            res = conn.execute(
                text("DELETE FROM operation_data WHERE `ç¾å›¢é—¨åº—ID` = :sid AND DATE(`æ—¥æœŸ`) BETWEEN :s AND :e"),
                {"sid": sid, "s": min_date, "e": max_date}
            )
            print(f"âœ… åˆ é™¤ operation_data ä¸­ é—¨åº—ID={sid} {min_date}~{max_date} å…± {res.rowcount} æ¡ã€‚")

    existing_cols = reflect_existing_columns("operation_data")
    flat_cols = [c for c in existing_cols if c in df_all.columns]
    df_basic = df_all[flat_cols].copy()
    dynamic_df = df_all.drop(columns=flat_cols, errors="ignore")
    dicts = dynamic_df.to_dict(orient="records")
    cleaned = [{k: (None if pd.isna(v) else v) for k, v in d.items()} for d in dicts]
    df_basic["extra_metrics"] = [json.dumps(d, ensure_ascii=False) for d in cleaned]
    if "ROSåˆ†" in df_all.columns:
        df_basic["ros_score"] = pd.to_numeric(df_all["ROSåˆ†"], errors="coerce").fillna(0).astype(int)
    df_basic["rankings_detail"] = df_all.apply(_build_rankings_detail, axis=1)

    dtype_op = get_dtype_for_operation(df_basic)
    import_to_mysql(df_basic, "operation_data", dtype=dtype_op)
    print(f"âœ… æˆåŠŸå¯¼å…¥è¿è¥æ•°æ®ï¼Œå…± {len(df_basic)} è¡Œã€‚")
    
    # å¯¼å…¥æˆåŠŸåæ¸…ç†æ–‡ä»¶
    print("ğŸ—‘ï¸ å¼€å§‹æ¸…ç†å·²å¯¼å…¥çš„è¿è¥æ•°æ®æ–‡ä»¶...")
    for fp in [os.path.join(folder, fname) for fname in os.listdir(folder) if fname.endswith(".xlsx")]:
        try:
            # å°è¯•ç§»åŠ¨åˆ°å›æ”¶ç«™ï¼Œå¦‚æœå¤±è´¥åˆ™åˆ é™¤
            try:
                from send2trash import send2trash
                send2trash(fp)
                print(f"ğŸ—‘ï¸ å·²ç§»å…¥å›æ”¶ç«™ï¼š{fp}")
            except ImportError:
                os.remove(fp)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ï¼š{fp}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ–‡ä»¶å¤±è´¥ {fp}: {e}")


def import_cpc_folder(folder: str) -> None:
    print(f"ğŸ“‚ æ­£åœ¨è¯»å–å°æ—¶çº§CPCæ•°æ®è·¯å¾„ï¼š{folder}")
    os.makedirs(folder, exist_ok=True)

    engine = get_engine()
    store_mapping = pd.read_sql("SELECT * FROM store_mapping", con=engine)
    dfs = []
    for fname in os.listdir(folder):
        if not fname.endswith(".xlsx"):
            continue
        fp = os.path.join(folder, fname)
        try:
            df = clean_and_load_excel(fp)
            df = process_cpc_dates(df, fname)
            df = match_store_id_for_single_cpc(df, store_mapping)
            df = drop_percentage_columns(df)
            df = clean_numeric_columns(df, key_col='é—¨åº—åç§°')
            df = add_datetime_column(df)
            dfs.append(df)
        except Exception as e:
            print(f"âŒ CPCæ•°æ®æ–‡ä»¶ {fp} å¤„ç†å¤±è´¥: {e}")
    if not dfs:
        print("âš ï¸ æ— CPCæ•°æ®æ–‡ä»¶å¯¼å…¥"); return

    df_all = pd.concat(dfs, ignore_index=True)
    df_all['plan_key'] = (
        df_all['é—¨åº—ID'].astype(str).str.strip()
        + "_"
        + df_all['æ¨å¹¿åç§°'].astype(str).str.strip()
        + "_"
        + df_all['å¹³å°'].astype(str).str.strip()
    )
    # ä¸è€ä»£ç ä¸€è‡´ï¼šé‡å‘½åååŒ…å« 'start_time' åˆ—
    df_all.rename(columns=COLUMN_MAPPING_CPC_HOURLY, inplace=True)
    # å»é‡é”®ä½¿ç”¨ 'plan_key' + 'start_time'
    if 'start_time' not in df_all.columns:
        print("âš ï¸ CPCæ•°æ®ç¼ºå°‘ start_time åˆ—ï¼Œæ£€æŸ¥åŸå§‹è¡¨å¤´æ˜¯å¦åŒ…å«â€˜æ—¶æ®µâ€™å¹¶å·²æˆåŠŸè§£æä¸ºâ€˜èµ·å§‹æ—¶é—´â€™ â†’ start_timeã€‚")
    df_all.drop_duplicates(subset=['plan_key', 'start_time'], inplace=True)
    # è€ä»£ç åœ¨é‡å‘½ååä»å°† 'date' è§£æä¸ºæ—¥æœŸ
    if 'date' in df_all.columns:
        df_all['date'] = pd.to_datetime(df_all['date']).dt.date
    min_date, max_date = df_all['date'].min(), df_all['date'].max()
    store_ids = df_all['store_id'].astype(str).unique().tolist()

    with engine.begin() as conn:
        for sid in store_ids:
            res = conn.execute(
                text("DELETE FROM cpc_hourly_data WHERE store_id = :sid AND DATE(date) BETWEEN :s AND :e"),
                {"sid": sid, "s": min_date, "e": max_date}
            )
            print(f"âœ… åˆ é™¤ cpc_hourly_data ä¸­ é—¨åº—ID={sid} {min_date}~{max_date} å…± {res.rowcount} æ¡ã€‚")

    dtype_cpc = get_dtype_for_cpc_hourly(df_all)
    import_to_mysql(df_all, "cpc_hourly_data", dtype=dtype_cpc)
    print(f"âœ… æˆåŠŸå¯¼å…¥å°æ—¶çº§CPCæ•°æ®ï¼Œå…± {len(df_all)} è¡Œã€‚")
    
    # å¯¼å…¥æˆåŠŸåæ¸…ç†æ–‡ä»¶
    print("ğŸ—‘ï¸ å¼€å§‹æ¸…ç†å·²å¯¼å…¥çš„CPCæ•°æ®æ–‡ä»¶...")
    for fp in [os.path.join(folder, fname) for fname in os.listdir(folder) if fname.endswith(".xlsx")]:
        try:
            # å°è¯•ç§»åŠ¨åˆ°å›æ”¶ç«™ï¼Œå¦‚æœå¤±è´¥åˆ™åˆ é™¤
            try:
                from send2trash import send2trash
                send2trash(fp)
                print(f"ğŸ—‘ï¸ å·²ç§»å…¥å›æ”¶ç«™ï¼š{fp}")
            except ImportError:
                os.remove(fp)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ï¼š{fp}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ–‡ä»¶å¤±è´¥ {fp}: {e}")


