"""
æ—¥æŠ¥ç”Ÿæˆä¸»å…¥å£è„šæœ¬
åŠŸèƒ½ï¼šä»æ•°æ®åº“æ‹‰å–æœ€æ–°æ•°æ®ï¼Œç”Ÿæˆæ—¥æŠ¥å¹¶è¾“å‡ºåˆ°æŒ‡å®šç›®å½•ã€‚
æ‰€æœ‰è·¯å¾„ã€å“ç‰Œæ˜ å°„ç­‰é…ç½®å‡å¼•ç”¨ configã€‚
"""
import os
import json
import pandas as pd
import time
from datetime import datetime, timedelta
from pathlib import Path
from sqlalchemy import create_engine
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.config import db_config, REPORT_OUTPUT_DIR
from utils.logger import get_logger

# æ•°æ®åº“è¿æ¥
DB_CONNECTION_STRING = f'mysql+pymysql://{db_config["user"]}:{db_config["password"]}@{db_config["host"]}:{db_config["port"]}/{db_config["database"]}'
engine = create_engine(DB_CONNECTION_STRING)

# é…ç½®å‚æ•°
THRESHOLD = 30   # æš´æ¶¨/æš´è·Œåˆ¤å®šé˜ˆå€¼ %
CORE_FIELDS = ["æ¶ˆè´¹é‡‘é¢", "æ‰“å¡äººæ•°", "æ–°å¢æ”¶è—äººæ•°", "æ–°å¥½è¯„æ•°", "æ–°ä¸­å·®è¯„æ•°"]
RANK_PLATFORMS = {
    "dianping_hot": "ç‚¹è¯„çƒ­é—¨æ¦œ",
    "dianping_checkin": "ç‚¹è¯„æ‰“å¡äººæ°”æ¦œ",
    "dianping_rating":"ç‚¹è¯„å¥½è¯„æ¦œ"
}

def get_store_mapping():
    """è·å–é—¨åº—æ˜ å°„æ•°æ®"""
    try:
        store_map = pd.read_sql(
            "SELECT é—¨åº—ID AS store_id, ç¾å›¢é—¨åº—ID, æ¨å¹¿é—¨åº— AS brand_name, è¿è¥å¸ˆ AS operator FROM store_mapping",
            engine
        )
        return store_map
    except Exception as e:
        logger = get_logger('generate_daily_report')
        logger.error(f"è·å–é—¨åº—æ˜ å°„å¤±è´¥: {e}")
        return pd.DataFrame()

def summarize(df, fields):
    """
    æ ¹æ®å­—æ®µæ±‡æ€»æ•°æ®ï¼Œå¦‚æœå­—æ®µç¼ºå¤±åˆ™è·³è¿‡
    """
    summary = {}
    for col in fields:
        if col not in df.columns:
            continue
        # æ•°å€¼å‹å­—æ®µæ±‚å’Œï¼Œå…¶ä»–å­—æ®µå–å¹³å‡å€¼
        if any(x in col for x in ['é‡‘é¢', 'äººæ•°', 'æ¬¡æ•°', 'ç¬”æ•°', 'æ•°']):
            value = df[col].sum()
        else:
            value = df[col].mean()
        summary[col] = round(value, 2) if isinstance(value, float) else value
    return summary

def compute_cpc_contribution_ratios(op_summary, cpc_summary):
    """è®¡ç®—CPCè´¡çŒ®æ¯”ä¾‹"""
    def safe_div(n, d, precision=2):
        return round(n / d * 100, precision) if d else None

    clicks = cpc_summary.get("clicks", 0)
    impressions = cpc_summary.get("impressions", 0)
    cost = cpc_summary.get("cost", 0)
    orders = cpc_summary.get("orders", 0)

    ratios = {
        "æ›å…‰å æ¯”": safe_div(impressions, op_summary.get("æ›å…‰äººæ•°", 0)),
        "ç‚¹å‡»å æ¯”": safe_div(clicks, op_summary.get("è®¿é—®äººæ•°", 0)),
        "å¹³å‡ç‚¹å‡»å•ä»·": round(cost / clicks, 4) if clicks else 0,
        "CPA": round(cost / orders, 2) if orders else None,
        "CTR": safe_div(clicks, impressions),
        "CVR": safe_div(orders, clicks)
    }
    return ratios

def is_big_change(pct_str, threshold=THRESHOLD):
    """åˆ¤æ–­æ˜¯å¦ä¸ºå¤§å¹…å˜åŒ–"""
    try:
        return abs(float(pct_str.strip('%'))) >= threshold
    except:
        return False

def fmt(val, pct_str):
    """æ ¼å¼åŒ–æ•°å€¼å’Œç™¾åˆ†æ¯”"""
    if pct_str == "N/A":
        return str(val)
    pct = float(pct_str.strip('%'))
    if pct > 0:
        return f"{val}(+{pct}%)"
    if pct < 0:
        return f"{val}(-{abs(pct)}%)"
    return f"{val}(æŒå¹³)"

# ==================== å›¾è¡¨ç”ŸæˆåŠŸèƒ½ï¼ˆå·²æ³¨é‡Šï¼‰ ====================
# TODO: ç”¨æˆ·è¦æ±‚æš‚æ—¶å¿½ç•¥å›¾ç‰‡åŠŸèƒ½ï¼Œåç»­ä¼šå¤§æ”¹
# å¦‚éœ€é‡æ–°å¯ç”¨ï¼Œè¯·å–æ¶ˆä»¥ä¸‹æ³¨é‡Šå¹¶åˆ é™¤ pass è¯­å¥
def plot_vertical_table(brand, report_date, engine):
    """ç”Ÿæˆå“ç‰Œæœ€è¿‘7å¤©å…³é”®æŒ‡æ ‡å›¾è¡¨ï¼ˆå·²ç¦ç”¨ï¼‰"""
    pass
    # åŸå§‹ä»£ç å·²æ³¨é‡Šï¼Œé‡æ–°å¯ç”¨æ—¶è¯·å–æ¶ˆæ³¨é‡Šä»¥ä¸‹ä»£ç å—
    """
    logger = get_logger('generate_daily_report')
    try:
        import matplotlib.pyplot as plt
        from matplotlib import font_manager
        
        # æ³¨å†Œä¸­æ–‡å­—ä½“
        try:
            font_manager.fontManager.addfont("./fonts/SimHei.ttf")
            plt.rcParams['font.family'] = 'SimHei'
        except:
            plt.rcParams['font.family'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # å–æœ€è¿‘7å¤©æ•°æ®
        start = report_date - timedelta(days=6)
        df = pd.read_sql(f'''
            SELECT 
                `æ—¥æœŸ`,
                `æ¶ˆè´¹é‡‘é¢` AS `æ¶ˆè´¹é‡‘é¢`,
                `æ›å…‰äººæ•°`,
                `è®¿é—®äººæ•°`,
                `è´­ä¹°äººæ•°`,
                `æ‰“å¡äººæ•°`,
                `æ‰«ç äººæ•°`,
                `æ–°å¢æ”¶è—äººæ•°`,
                `æ–°å¥½è¯„æ•°`,
                `æ–°ä¸­å·®è¯„æ•°`,
                `ç‚¹è¯„æ˜Ÿçº§` AS `æ˜Ÿçº§`
            FROM `operation_data`
            WHERE `ç¾å›¢é—¨åº—ID` IN (
                SELECT `ç¾å›¢é—¨åº—ID` 
                  FROM `store_mapping` 
                 WHERE `æ¨å¹¿é—¨åº—` = '{brand}'
            )
              AND `æ—¥æœŸ` BETWEEN '{start.date()}' AND '{report_date.date()}'
            ORDER BY `æ—¥æœŸ`
        ''', engine)
        
        if df.empty:
            logger.warning(f"âš ï¸ {brand} æœ€è¿‘7å¤©æ— æ•°æ®ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
            return
            
        # æŠŠæ—¥æœŸåˆ—è½¬æˆçº¯ YYYY-MM-DD çš„å­—ç¬¦ä¸²
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ']).dt.strftime('%Y-%m-%d')
        
        # è¡ç”Ÿæ˜ŸæœŸå‡ 
        weekday_map = {0:'æ˜ŸæœŸä¸€',1:'æ˜ŸæœŸäºŒ',2:'æ˜ŸæœŸä¸‰',3:'æ˜ŸæœŸå››',
                       4:'æ˜ŸæœŸäº”',5:'æ˜ŸæœŸå…­',6:'æ˜ŸæœŸæ—¥'}
        df['æ˜ŸæœŸ'] = pd.to_datetime(df['æ—¥æœŸ']).dt.weekday.map(weekday_map)
        
        # å¼ºåˆ¶æŠŠæ‰€æœ‰"äººæ•°"åˆ—å’Œè¯„åˆ†åˆ—å˜æˆæ•´æ•°
        int_cols = [
            'æ›å…‰äººæ•°','è®¿é—®äººæ•°','è´­ä¹°äººæ•°',
            'æ‰“å¡äººæ•°','æ‰«ç äººæ•°','æ–°å¢æ”¶è—äººæ•°',
            'æ–°å¥½è¯„æ•°','æ–°ä¸­å·®è¯„æ•°'
        ]
        df[int_cols] = df[int_cols].astype(int)
        
        # è®¡ç®—ä¸¤æ®µè½¬åŒ–ç‡
        df['æ›å…‰-è®¿é—®è½¬åŒ–ç‡'] = (df['è®¿é—®äººæ•°'] / df['æ›å…‰äººæ•°'] * 100).round(1).astype(str) + '%'
        df['è®¿é—®-è´­ä¹°è½¬åŒ–ç‡'] = (df['è´­ä¹°äººæ•°'] / df['è®¿é—®äººæ•°'] * 100).round(1).astype(str) + '%'
        
        # æœ€ç»ˆé€‰åˆ— & æ’åºï¼Œå¹¶é‡å‘½åä¸ºçŸ­æ ‡é¢˜
        cols = [
            'æ—¥æœŸ', 'æ˜ŸæœŸ', 'æ¶ˆè´¹é‡‘é¢', 'æ›å…‰äººæ•°', 'è®¿é—®äººæ•°', 'è´­ä¹°äººæ•°',
            'æ›å…‰-è®¿é—®è½¬åŒ–ç‡', 'è®¿é—®-è´­ä¹°è½¬åŒ–ç‡',
            'æ–°å¢æ”¶è—äººæ•°', 'æ‰“å¡äººæ•°',
            'æ–°å¥½è¯„æ•°', 'æ–°ä¸­å·®è¯„æ•°', 'æ‰«ç äººæ•°', 'æ˜Ÿçº§'
        ]
        df = df[cols].rename(columns={
            'æ¶ˆè´¹é‡‘é¢': 'æ¶ˆè´¹',
            'æ›å…‰äººæ•°': 'æ›å…‰',
            'è®¿é—®äººæ•°': 'è®¿é—®',
            'è´­ä¹°äººæ•°': 'è´­ä¹°',
            'æ›å…‰-è®¿é—®è½¬åŒ–ç‡': 'è®¿é—®è½¬åŒ–',
            'è®¿é—®-è´­ä¹°è½¬åŒ–ç‡': 'è´­ä¹°è½¬åŒ–',
            'æ–°å¢æ”¶è—äººæ•°': 'æ”¶è—',
            'æ‰“å¡äººæ•°': 'æ‰“å¡',
            'æ–°å¥½è¯„æ•°': 'å¥½è¯„',
            'æ–°ä¸­å·®è¯„æ•°': 'å·®è¯„',
            'æ‰«ç äººæ•°': 'æ‰«ç ',
            'æ˜Ÿçº§':'æ˜Ÿçº§'
        })
        
        # ç»˜åˆ¶è¡¨æ ¼
        col_widths = []
        for c in df.columns:
            if c in ['æ—¥æœŸ', 'æ˜ŸæœŸ']:
                col_widths.append(0.08)
            elif c in ['æ¶ˆè´¹', 'æ›å…‰', 'è®¿é—®', 'è´­ä¹°', 'è®¿é—®è½¬åŒ–', 'è´­ä¹°è½¬åŒ–']:
                col_widths.append(0.06)
            else:
                col_widths.append(0.05)
        
        fig, ax = plt.subplots(figsize=(16, 0.6 * len(df) + 1))
        ax.axis('off')
        tbl = ax.table(
            cellText=df.values,
            colLabels=df.columns,
            colWidths=col_widths,
            cellLoc='center',
            loc='center'
        )
        tbl.auto_set_font_size(False)
        tbl.set_fontsize(11)
        tbl.scale(1, 1.2)
        
        ax.set_title(f"{brand} æœ€è¿‘7å¤©å…³é”®æŒ‡æ ‡", fontsize=16, pad=12)
        plt.tight_layout()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        out_dir = Path(REPORT_OUTPUT_DIR)
        out_dir.mkdir(exist_ok=True)
        out = out_dir / f"{brand}_æœ€è¿‘7å¤©æŒ‡æ ‡è¡¨.png"
        fig.savefig(out, dpi=150, bbox_inches='tight')
        plt.close(fig)
        logger.info(f"âœ… ä¿å­˜å›¾è¡¨ï¼š{out}")
        
    except ImportError:
        logger.warning("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
    except Exception as e:
        logger.error(f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")
    """

def analyze_brand_performance(brand, df_op, op_sum, link_ratio):
    """åˆ†æå“ç‰Œè¡¨ç°å¹¶ç»™å‡ºä»·å€¼æ´å¯Ÿ"""
    insights = []
    
    # æ¶ˆè´¹é‡‘é¢åˆ†æ
    rev_val = round(df_op.iloc[0]['æ¶ˆè´¹é‡‘é¢'], 0)
    rev_pct = link_ratio.get('æ¶ˆè´¹é‡‘é¢', 'N/A')
    if rev_pct != 'N/A':
        rev_change = float(rev_pct.strip('%'))
        if rev_change > 20:
            insights.append("ğŸ’° æ¶ˆè´¹é‡‘é¢å¤§å¹…å¢é•¿ï¼Œè¡¨ç°ä¼˜ç§€")
        elif rev_change < -20:
            insights.append("âš ï¸ æ¶ˆè´¹é‡‘é¢ä¸‹é™æ˜æ˜¾ï¼Œéœ€è¦å…³æ³¨")
    
    # è½¬åŒ–ç‡åˆ†æ
    exposure = df_op.iloc[0]['æ›å…‰äººæ•°']
    visits = df_op.iloc[0]['è®¿é—®äººæ•°']
    purchases = df_op.iloc[0]['è´­ä¹°äººæ•°']
    
    if exposure > 0 and visits > 0:
        visit_rate = visits / exposure
        if visit_rate < 0.05:
            insights.append("ğŸ“‰ è®¿é—®è½¬åŒ–ç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–é¡µé¢")
        elif visit_rate > 0.15:
            insights.append("ğŸ“ˆ è®¿é—®è½¬åŒ–ç‡ä¼˜ç§€")
    
    if visits > 0 and purchases > 0:
        purchase_rate = purchases / visits
        if purchase_rate < 0.02:
            insights.append("ğŸ›’ è´­ä¹°è½¬åŒ–ç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–å•†å“å±•ç¤º")
        elif purchase_rate > 0.08:
            insights.append("ğŸ¯ è´­ä¹°è½¬åŒ–ç‡ä¼˜ç§€")
    
    # è¯„ä»·åˆ†æ
    good_val = int(df_op.iloc[0]['æ–°å¥½è¯„æ•°'])
    bad_val = int(df_op.iloc[0]['æ–°ä¸­å·®è¯„æ•°'])
    
    if good_val > 0 and bad_val == 0:
        insights.append("â­ å¥½è¯„ç¨³å®šï¼Œæ— å·®è¯„ï¼ŒæœåŠ¡è¡¨ç°ä¼˜ç§€")
    elif bad_val > 0:
        insights.append("âš ï¸ å‡ºç°å·®è¯„ï¼Œéœ€è¦åŠæ—¶è·Ÿè¿›å¤„ç†")
    
    # æ”¶è—åˆ†æ
    col_val = int(op_sum.get("æ–°å¢æ”¶è—äººæ•°", 0))
    col_pct = link_ratio.get("æ–°å¢æ”¶è—äººæ•°", "N/A")
    if col_pct != 'N/A':
        col_change = float(col_pct.strip('%'))
        if col_change > 50:
            insights.append("â¤ï¸ æ”¶è—å¢é•¿æ˜¾è‘—ï¼Œç”¨æˆ·ç²˜æ€§æå‡")
        elif col_change < -50:
            insights.append("ğŸ“‰ æ”¶è—ä¸‹é™æ˜æ˜¾ï¼Œéœ€è¦å…³æ³¨ç”¨æˆ·ç•™å­˜")
    
    return insights

# ==================== å¯¹æ¯”è¡¨ç”ŸæˆåŠŸèƒ½ï¼ˆå·²æ³¨é‡Šï¼‰ ====================
# TODO: ç”¨æˆ·è¦æ±‚æš‚æ—¶å¿½ç•¥å›¾ç‰‡åŠŸèƒ½ï¼Œåç»­ä¼šå¤§æ”¹
# å¦‚éœ€é‡æ–°å¯ç”¨ï¼Œè¯·å–æ¶ˆä»¥ä¸‹æ³¨é‡Šå¹¶åˆ é™¤ pass è¯­å¥
def plot_comparison_table(brand, report_date):
    """ç”Ÿæˆå“ç‰ŒåŒæœŸå¯¹æ¯”è¡¨æ ¼ï¼ˆå·²ç¦ç”¨ï¼‰"""
    pass
    # åŸå§‹ä»£ç å·²æ³¨é‡Šï¼Œé‡æ–°å¯ç”¨æ—¶è¯·å–æ¶ˆæ³¨é‡Šä»¥ä¸‹ä»£ç å—
    """
    logger = get_logger('generate_daily_report')
    try:
        import matplotlib.pyplot as plt
        from matplotlib import font_manager
        
        # æ³¨å†Œä¸­æ–‡å­—ä½“
        try:
            font_manager.fontManager.addfont("./fonts/SimHei.ttf")
            plt.rcParams['font.family'] = 'SimHei'
        except:
            plt.rcParams['font.family'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # ä»store_mapå–å‡ºæœ¬å“ç‰Œæ‰€æœ‰ç¾å›¢é—¨åº—ID
        mids = store_map.loc[store_map['brand_name'] == brand, 'ç¾å›¢é—¨åº—ID'].astype(str).tolist()
        if not mids:
            logger.warning(f"âš ï¸ å“ç‰Œ {brand} æ— å¯¹åº”ç¾å›¢é—¨åº—IDï¼Œè·³è¿‡å¯¹æ¯”è¡¨")
            return
        mids_sql = ",".join(f"'{m}'" for m in mids)
        
        # è®¡ç®—"æœ¬æœŸ""ä¸ŠæœŸ"æ—¥æœŸ
        wd = report_date.weekday()
        if wd == 0:
            # å‘¨ä¸€ï¼šæœ¬æœŸ = å‘¨äº”~å‘¨æ—¥ï¼Œ ä¸ŠæœŸ = ä¸Šå‘¨å‘¨äº”~å‘¨æ—¥
            curr_start = report_date - timedelta(days=3)
            prev_start = report_date - timedelta(days=10)
            current_dates = [d.date() for d in pd.date_range(curr_start, periods=3)]
            prev_dates = [d.date() for d in pd.date_range(prev_start, periods=3)]
            label_curr, label_prev = "æœ¬æœŸ(å‘¨äº”~å‘¨æ—¥)", "ä¸ŠæœŸ(ä¸Šå‘¨å‘¨äº”~å‘¨æ—¥)"
        else:
            # å‘¨äºŒ~å‘¨äº”ï¼šæœ¬æœŸ = æ˜¨æ—¥, ä¸ŠæœŸ = ä¸Šå‘¨åŒæœŸ
            current_dates = [(report_date - timedelta(days=1)).date()]
            prev_dates = [(report_date - timedelta(days=8)).date()]
            label_curr, label_prev = "æ˜¨æ—¥", "ä¸Šå‘¨åŒæœŸ"
        
        # æ‹‰å–æ•°æ®
        metrics = [
            "æ›å…‰äººæ•°","è®¿é—®äººæ•°","è´­ä¹°äººæ•°","æ¶ˆè´¹é‡‘é¢",
            "æ–°å¥½è¯„æ•°","æ–°ä¸­å·®è¯„æ•°","æ‰“å¡äººæ•°","æ‰«ç äººæ•°","æ–°å¢æ”¶è—äººæ•°","ç‚¹è¯„æ˜Ÿçº§"
        ]
        cols = ", ".join(f"`{m}`" for m in metrics)
        cd_sql = ", ".join(f"'{d}'" for d in current_dates)
        pd_sql = ", ".join(f"'{d}'" for d in prev_dates)
        
        df_curr = pd.read_sql(
            f"SELECT `æ—¥æœŸ`, {cols} FROM `operation_data` "
            f"WHERE `ç¾å›¢é—¨åº—ID` IN ({mids_sql}) AND `æ—¥æœŸ` IN ({cd_sql})",
            engine
        ).set_index("æ—¥æœŸ")
        
        df_prev = pd.read_sql(
            f"SELECT `æ—¥æœŸ`, {cols} FROM `operation_data` "
            f"WHERE `ç¾å›¢é—¨åº—ID` IN ({mids_sql}) AND `æ—¥æœŸ` IN ({pd_sql})",
            engine
        ).set_index("æ—¥æœŸ")
        
        # æ±‡æ€»å¹¶æ„é€ å¯¹æ¯”è¡¨
        if df_curr.empty or df_prev.empty:
            logger.warning(f"âš ï¸ å“ç‰Œ {brand} æœ¬æœŸæˆ–ä¸ŠæœŸæ— æ•°æ®ï¼Œè·³è¿‡å¯¹æ¯”è¡¨")
            return
            
        curr_sum = df_curr.sum()
        prev_sum = df_prev.sum()
        df_cmp = pd.DataFrame({
            "æŒ‡æ ‡": metrics,
            label_curr: [curr_sum[m] for m in metrics],
            label_prev: [prev_sum[m] for m in metrics],
        })
        df_cmp["å˜åŒ–ç‡"] = (
            (df_cmp[label_curr] - df_cmp[label_prev]) / df_cmp[label_prev] * 100
        ).round(1).astype(str) + "%"
        
        # å±•ç¤ºå¹¶ä¿å­˜
        fig, ax = plt.subplots(figsize=(len(df_cmp)*0.8, 2.5))
        ax.axis("off")
        tbl = ax.table(
            cellText=df_cmp.values,
            colLabels=df_cmp.columns,
            loc="center"
        )
        tbl.auto_set_font_size(False)
        tbl.set_fontsize(12)
        tbl.scale(1, 1.5)
        ax.set_title(brand, fontsize=16, pad=10)
        plt.tight_layout()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        out_dir = Path(REPORT_OUTPUT_DIR)
        out_dir.mkdir(exist_ok=True)
        out_path = out_dir / f"{brand}_åŒæœŸå¯¹æ¯”è¡¨.png"
        fig.savefig(out_path, dpi=150, bbox_inches="tight")
        plt.close(fig)
        logger.info(f"âœ… ä¿å­˜å¯¹æ¯”è¡¨ï¼š{out_path}")
        
    except ImportError:
        logger.warning("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å¯¹æ¯”è¡¨ç”Ÿæˆ")
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¯¹æ¯”è¡¨å¤±è´¥: {e}")
    """

def generate_daily_report(report_date, output_dir):
    """
    ç”Ÿæˆæ—¥æŠ¥å¹¶è¾“å‡ºåˆ°æŒ‡å®šç›®å½•
    :param report_date: æŠ¥å‘Šæ—¥æœŸ
    :param output_dir: è¾“å‡ºç›®å½•
    """
    logger = get_logger('generate_daily_report')
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    out_dir = Path(output_dir)
    out_dir.mkdir(exist_ok=True)
    
    # è·å–é—¨åº—æ˜ å°„
    store_map = get_store_mapping()
    if store_map.empty:
        logger.error("æ— æ³•è·å–é—¨åº—æ˜ å°„æ•°æ®ï¼Œé€€å‡º")
        return
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    last_7_start = report_date - timedelta(days=7)
    last_14_start = report_date - timedelta(days=14)
    
    logger.info(f"å¼€å§‹ç”Ÿæˆ {report_date.date()} çš„æ—¥æŠ¥")
    
    try:
        # æ‹‰å–æ˜¨æ—¥è¿è¥æ•°æ®
        logger.info("æ‹‰å–æ˜¨æ—¥è¿è¥æ•°æ®")
        op_today = pd.read_sql(
            f"""SELECT 
                   `ç¾å›¢é—¨åº—ID`,`æ›å…‰äººæ•°`,`è®¿é—®äººæ•°`,`è´­ä¹°äººæ•°`,
                   `æ¶ˆè´¹é‡‘é¢`,`æ–°å¥½è¯„æ•°`,`æ–°ä¸­å·®è¯„æ•°`,
                   `æ‰“å¡äººæ•°`,`æ‰«ç äººæ•°`,`ç‚¹è¯„æ˜Ÿçº§`,`æ–°å¢æ”¶è—äººæ•°`,`rankings_detail`
               FROM `operation_data`
               WHERE `æ—¥æœŸ` = '{report_date.date()}'""",
            engine
        ).merge(
            store_map[['ç¾å›¢é—¨åº—ID','brand_name','operator']],
            on='ç¾å›¢é—¨åº—ID', how='left'
        ).rename(columns={'brand_name':'æ¨å¹¿é—¨åº—'})

        # æ‹‰å–è¿‘7å¤©è¿è¥æ•°æ®
        logger.info("æ‹‰å–è¿‘7å¤©è¿è¥æ•°æ®")
        op_last7 = pd.read_sql(
            f"""SELECT
                   `ç¾å›¢é—¨åº—ID`,`æ—¥æœŸ`,`æ›å…‰äººæ•°`,`è®¿é—®äººæ•°`,`è´­ä¹°äººæ•°`,
                   `æ¶ˆè´¹é‡‘é¢`,`æ–°å¥½è¯„æ•°`,`æ–°ä¸­å·®è¯„æ•°`,
                   `æ‰“å¡äººæ•°`,`æ‰«ç äººæ•°`,`ç‚¹è¯„æ˜Ÿçº§`,`æ–°å¢æ”¶è—äººæ•°`
               FROM `operation_data`
               WHERE `æ—¥æœŸ` BETWEEN '{last_7_start.date()}' AND '{report_date.date()}'""",
            engine
        ).merge(
            store_map[['ç¾å›¢é—¨åº—ID','brand_name','operator']],
            on='ç¾å›¢é—¨åº—ID', how='left'
        ).rename(columns={'brand_name':'æ¨å¹¿é—¨åº—'})

        # æ‹‰å–æ˜¨æ—¥CPCæ•°æ®
        logger.info("æ‹‰å–æ˜¨æ—¥CPCæ•°æ®")
        cpc_today = pd.read_sql(
            f"""SELECT `store_id`,`cost`,`impressions`,`clicks`,`orders`
               FROM `cpc_hourly_data`
               WHERE `date` = '{report_date.date()}'""",
            engine
        ).merge(
            store_map[['store_id','brand_name','operator']],
            on='store_id', how='left'
        ).rename(columns={'brand_name':'æ¨å¹¿é—¨åº—'})

        # æ‹‰å–æœ€è¿‘14å¤©å†å²æ•°æ®
        logger.info("æ‹‰å–æœ€è¿‘14å¤©è¿è¥æ•°æ®")
        op_hist = pd.read_sql(
            f"""SELECT 
                   `ç¾å›¢é—¨åº—ID`,`æ—¥æœŸ`,`æ›å…‰äººæ•°`,`è®¿é—®äººæ•°`,`è´­ä¹°äººæ•°`,
                   `æ¶ˆè´¹é‡‘é¢`,`æ–°å¥½è¯„æ•°`,`æ–°ä¸­å·®è¯„æ•°`,
                   `æ‰“å¡äººæ•°`,`æ‰«ç äººæ•°`,`ç‚¹è¯„æ˜Ÿçº§`,`æ–°å¢æ”¶è—äººæ•°`
               FROM `operation_data`
               WHERE `æ—¥æœŸ` BETWEEN '{last_14_start.date()}' AND '{report_date.date()}'""",
            engine
        ).merge(
            store_map[['ç¾å›¢é—¨åº—ID','brand_name','operator']],
            on='ç¾å›¢é—¨åº—ID', how='left'
        ).rename(columns={'brand_name':'æ¨å¹¿é—¨åº—'})

        # åˆ†ç»„å‡†å¤‡
        op_group = op_today.groupby("æ¨å¹¿é—¨åº—")
        op7_group = op_last7.groupby("æ¨å¹¿é—¨åº—")
        cpc_group = cpc_today.groupby("æ¨å¹¿é—¨åº—")

        # ==================== å›¾è¡¨ç”Ÿæˆè°ƒç”¨ï¼ˆå·²æ³¨é‡Šï¼‰ ====================
        # TODO: ç”¨æˆ·è¦æ±‚æš‚æ—¶å¿½ç•¥å›¾ç‰‡åŠŸèƒ½ï¼Œåç»­ä¼šå¤§æ”¹
        # å¦‚éœ€é‡æ–°å¯ç”¨ï¼Œè¯·å–æ¶ˆä»¥ä¸‹æ³¨é‡Š
        """
        # å…ˆä¸ºæ¯å®¶é—¨åº—ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
        logger.info("å¼€å§‹ç”Ÿæˆå›¾è¡¨...")
        for brand in op_group.groups.keys():
            plot_vertical_table(brand, report_date, engine)
            plot_comparison_table(brand, report_date)
        """

        # æŒ‰è¿è¥å¸ˆæ”¶é›†æŠ¥å‘Šå†…å®¹
        from collections import defaultdict
        operator_sections = defaultdict(list)

        # æŒ‡æ ‡å­—æ®µ
        op_fields = ["æ›å…‰äººæ•°","è®¿é—®äººæ•°","è´­ä¹°äººæ•°","æ¶ˆè´¹é‡‘é¢",
                    "æ–°å¥½è¯„æ•°","æ–°ä¸­å·®è¯„æ•°","æ‰“å¡äººæ•°","æ‰«ç äººæ•°","ç‚¹è¯„æ˜Ÿçº§","æ–°å¢æ”¶è—äººæ•°"]
        cpc_fields = ["cost","impressions","clicks","orders"]

        # ä¸ºæ¯ä¸ªå“ç‰Œç”ŸæˆæŠ¥å‘Š
        for brand, df_op in op_group:
            # å‡†å¤‡å½“æ—¥7å¤©å†å² & CPC & å…¨é‡å†å²æ•°æ®
            df_op7 = op7_group.get_group(brand) if brand in op7_group.groups else pd.DataFrame()
            df_cpc = cpc_group.get_group(brand) if brand in cpc_group.groups else pd.DataFrame()
            df_hist_b = op_hist[op_hist["æ¨å¹¿é—¨åº—"] == brand]

            # 1) æ±‡æ€»å½“æ—¥è¿è¥ & CPC æ•°æ®
            op_sum = summarize(df_op, op_fields)
            if df_cpc.empty:
                cpc_sum, ratios = {}, {}
            else:
                cpc_sum = summarize(df_cpc, cpc_fields)
                ratios = compute_cpc_contribution_ratios(op_sum, cpc_sum)

            # 2) è®¡ç®—æ—¥ç¯æ¯”ï¼ˆæ˜¨æ—¥ vs. ä¸Šå‘¨åŒæœŸï¼‰ï¼Œå‘¨ä¸€ç‰¹æ®Šå¤„ç†
            weekday = report_date.weekday()
            if weekday == 0:
                # å‘¨ä¸€ï¼šæŠŠ curr å®šä¹‰ä¸ºä¸Šå‘¨äº”~å‘¨æ—¥ï¼Œprev å®šä¹‰ä¸ºä¸Šä¸Šå‘¨äº”~å‘¨ä¸Šå‘¨æ—¥
                curr = df_hist_b[
                    (df_hist_b["æ—¥æœŸ"] >= (report_date - timedelta(days=3)).date()) &
                    (df_hist_b["æ—¥æœŸ"] <= (report_date - timedelta(days=1)).date())
                ]
                prev = df_hist_b[
                    (df_hist_b["æ—¥æœŸ"] >= (report_date - timedelta(days=10)).date()) &
                    (df_hist_b["æ—¥æœŸ"] <= (report_date - timedelta(days=8)).date())
                ]
                curr_sum = summarize(curr, op_fields)
            else:
                # éå‘¨ä¸€ï¼šæœ¬æœŸå°±æ˜¯æ˜¨å¤©ï¼Œprev æ˜¯ä¸Šå‘¨åŒä¸€å¤©
                curr = df_hist_b[df_hist_b["æ—¥æœŸ"] == (report_date - timedelta(days=1)).date()]
                prev = df_hist_b[df_hist_b["æ—¥æœŸ"] == (report_date - timedelta(days=8)).date()]
                curr_sum = op_sum

            prev_sum = summarize(prev, op_fields)

            # ç»„è£…ç¯æ¯”æ•°å­—
            link_ratio = {}
            for k in op_fields:
                if prev_sum.get(k, 0):
                    val_curr = curr_sum.get(k, 0)
                    val_prev = prev_sum.get(k, 0)
                    pct = round((val_curr - val_prev) / (val_prev or 1) * 100, 1)
                    link_ratio[k] = f"{pct}%"
                else:
                    link_ratio[k] = "N/A"

            # 3) è§£ææ¦œå•åŠ¨æ€
            non_null = df_op["rankings_detail"].dropna().tolist() if not df_op.empty else []
            raw_rank = non_null[0] if non_null else None
            if isinstance(raw_rank, (bytes, bytearray)):
                try: 
                    raw_rank = raw_rank.decode("utf-8")
                except: 
                    raw_rank = None
            if isinstance(raw_rank, str):
                try: 
                    parsed = json.loads(raw_rank)
                except: 
                    parsed = {}
            elif isinstance(raw_rank, dict):
                parsed = raw_rank
            else:
                parsed = {}
            if isinstance(parsed, str):
                try: 
                    tmp = json.loads(parsed)
                except: 
                    tmp = {}
                parsed = tmp if isinstance(tmp, dict) else {}
            rank_dict = parsed if isinstance(parsed, dict) else {}

            # åªä¿ç•™è¿™å‡ ä¸ªå¹³å°
            allowed = ["dianping_hot", "dianping_checkin", "dianping_rating"]
            th_city = 10
            th_sub = 5
            level_map = {"city": "å…¨å¸‚æ¦œ", "subdistrict": "åŒºå¿æ¦œ", "business": "å•†åœˆæ¦œ"}

            rank_lines = []
            for pf in allowed:
                scope = rank_dict.get(pf, {})
                if not isinstance(scope, dict):
                    continue
                desc = RANK_PLATFORMS[pf]
                city_rank = scope.get("city")
                sub_rank = scope.get("subdistrict")
                bus_rank = scope.get("business")
                
                if isinstance(city_rank, int) and city_rank <= th_city:
                    rank_lines.append(f"{desc}{level_map['city']}ç¬¬{city_rank}å")
                if isinstance(sub_rank, int) and sub_rank <= th_sub:
                    rank_lines.append(f"{desc}{level_map['subdistrict']}ç¬¬{sub_rank}å")
                if not (
                    (isinstance(city_rank, int) and city_rank <= th_city) or
                    (isinstance(sub_rank, int) and sub_rank <= th_sub)
                ) and isinstance(bus_rank, int):
                    rank_lines.append(f"{desc}{level_map['business']}ç¬¬{bus_rank}å")

            rank_note = "\n".join(rank_lines) if rank_lines else "æ— æ¦œå•å˜åŒ–"

            # 4) å–æ˜¨æ—¥å››é¡¹æ ¸å¿ƒæŒ‡æ ‡ & ç¯æ¯”
            today = df_op.iloc[0]
            card_val = int(today['æ‰“å¡äººæ•°'])
            good_val = int(today['æ–°å¥½è¯„æ•°'])
            bad_val = int(today['æ–°ä¸­å·®è¯„æ•°'])
            rev_val = round(today['æ¶ˆè´¹é‡‘é¢'], 0)

            card_pct = link_ratio.get('æ‰“å¡äººæ•°', 'N/A')
            good_pct = link_ratio.get('æ–°å¥½è¯„æ•°', 'N/A')
            bad_pct = link_ratio.get('æ–°ä¸­å·®è¯„æ•°', 'N/A')
            rev_pct = link_ratio.get('æ¶ˆè´¹é‡‘é¢', 'N/A')

            # æ ¼å¼åŒ–è¾“å‡º
            card_str = fmt(card_val, card_pct)
            good_str = fmt(good_val, good_pct)
            bad_str = "æš‚æ— å·®è¯„" if bad_val == 0 else fmt(bad_val, bad_pct)
            rev_str = fmt(rev_val, rev_pct)

            # æ–°å¢æ”¶è—
            col_val = int(op_sum.get("æ–°å¢æ”¶è—äººæ•°", 0))
            col_pct = link_ratio.get("æ–°å¢æ”¶è—äººæ•°", "N/A")
            col_str = fmt(col_val, col_pct)

            # å¼‚åŠ¨æ£€æµ‹
            extra_notes = []
            exp_pct = link_ratio.get("æ›å…‰äººæ•°", "N/A")
            if is_big_change(exp_pct):
                extra_notes.append(f"âš ï¸ æ›å…‰{exp_pct}")
            vis_pct = link_ratio.get("è®¿é—®äººæ•°", "N/A")
            if is_big_change(vis_pct):
                extra_notes.append(f"âš ï¸ è®¿é—®{vis_pct}")
            extra_note_str = "ï¼›" + "ï¼›".join(extra_notes) if extra_notes else ""

            # æ„å»ºæŠ¥å‘Šæ–‡æœ¬
            suggestion = ("å‘ç°å·®è¯„ï¼Œè¿è¥å¸ˆå·²ä»‹å…¥è·Ÿè¿›ã€‚" if bad_val > 0 else "æ•°æ®æ­£å¸¸ï¼Œç»§ç»­å¼•å¯¼å¥½è¯„ã€‚")
            
            # æ¨å¹¿é€šèŠ±è´¹åˆ†æ
            cpc_part = ""
            cost_today = cpc_sum.get("cost", 0) if not df_cpc.empty else 0
            cost_prev = 0  # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºè·å–ä¸Šå‘¨åŒæœŸCPCæ•°æ®
            if not (cost_today == 0 and cost_prev == 0):
                cpc_part = f"ï¼›æ¨å¹¿é€šèŠ±è´¹ {cost_today:.2f}"
                # åˆ¤æ–­æ¨å¹¿é€šèŠ±è´¹å¼‚å¸¸
                if (cost_prev > 0 and cost_today == 0) or (
                    cost_prev > 0 and abs(cost_today - cost_prev)/cost_prev >= 0.5
                    and abs(cost_today - cost_prev) >= 100
                ):
                    cpc_part += " âš ï¸ æ¨å¹¿é€šèŠ±è´¹å¼‚å¸¸"
            
            # æ•°æ®ä»·å€¼åˆ†æ
            insights = analyze_brand_performance(brand, df_op, op_sum, link_ratio)
            insights_text = "\n".join(insights) if insights else "æ•°æ®è¡¨ç°æ­£å¸¸"
            
            text = (
                f"{brand}\n"
                f"- æ ¸å¿ƒæŒ‡æ ‡ï¼šæ¶ˆè´¹ {rev_str}{cpc_part}ï¼›æ‰“å¡ {card_str}ï¼›æ”¶è— {col_str}ï¼›\n"
                f"  å¥½è¯„ {good_str}ï¼›{'' if bad_val == 0 else 'å·®è¯„ ' + bad_str}\n"
                f"- æ’è¡Œæ¦œï¼š\n{rank_note}\n"
                f"- æ•°æ®æ´å¯Ÿï¼š\n{insights_text}\n"
                f"- å»ºè®®ï¼š{suggestion}\n"
            )

            # æŒ‰è¿è¥å¸ˆæ”¶é›†
            match = store_map[store_map['brand_name'] == brand]
            if match.empty:
                logger.warning(f"å“ç‰Œ {brand} æœªåœ¨ store_map ä¸­åŒ¹é…åˆ°ï¼Œè·³è¿‡")
                continue
            op_val = match['operator'].values[0]
            if pd.isna(op_val) or not op_val:
                logger.warning(f"å“ç‰Œ {brand} æ‰¾åˆ°äº†ä½†è¿è¥å¸ˆå­—æ®µä¸ºç©ºï¼Œè·³è¿‡")
                continue

            logger.info(f"å“ç‰Œ {brand} åŒ¹é…è¿è¥å¸ˆï¼š{op_val}")
            operator_sections[op_val].append(text)

        # å†™å„è¿è¥å¸ˆçš„ TXT æ–‡ä»¶
        for op, texts in operator_sections.items():
            fn = out_dir / f"{op}_{report_date.date()}.txt"
            fn.write_text("\n\n".join(texts), encoding="utf-8-sig")
            logger.info(f"å·²ç”Ÿæˆè¿è¥å¸ˆæ—¥æŠ¥ï¼š{fn}")

        # ç”ŸæˆExcelæ–‡ä»¶
        logger.info("å¼€å§‹ç”ŸæˆExcelæœˆåº¦æ•°æ®æ–‡ä»¶")
        
        # æ‹‰å–å½“æœˆå…¨é‡æ•°æ®
        month_start = report_date.replace(day=1).date()
        df_month = pd.read_sql(
            f"""
            SELECT
              `ç¾å›¢é—¨åº—ID`,`æ—¥æœŸ`,`æ¶ˆè´¹é‡‘é¢`,`æ›å…‰äººæ•°`,`è®¿é—®äººæ•°`,`è´­ä¹°äººæ•°`,
              `æ‰«ç äººæ•°`,`æ–°å¢æ”¶è—äººæ•°`,`æ‰“å¡äººæ•°`,`æ–°å¥½è¯„æ•°`,`æ–°ä¸­å·®è¯„æ•°`,`ç‚¹è¯„æ˜Ÿçº§`
            FROM `operation_data`
            WHERE `æ—¥æœŸ` BETWEEN '{month_start}' AND '{report_date.date()}'
            """, engine
        ).merge(
            store_map[['ç¾å›¢é—¨åº—ID','brand_name','operator','store_id']],
            on='ç¾å›¢é—¨åº—ID', how='left'
        )

        # æ‹‰å–å½“æœˆæ¯æ—¥CPCæˆæœ¬
        cpc_month = pd.read_sql(
            f"""
            SELECT 
              store_id, `date` AS æ—¥æœŸ, SUM(cost) AS æ¨å¹¿é€šèŠ±è´¹
            FROM cpc_hourly_data
            WHERE `date` BETWEEN '{month_start}' AND '{report_date.date()}'
            GROUP BY store_id, `date`
            """, engine
        ).merge(
            store_map[['store_id','brand_name','operator']],
            on='store_id', how='left'
        )
        
        # åŒä¸€å¤©åŒå“ç‰Œå¯èƒ½å­˜åœ¨å¤šé—¨åº—ï¼ŒæŒ‰å“ç‰Œ+æ—¥æœŸæ±‡æ€»
        cpc_month = cpc_month.groupby(
            ['brand_name','operator','æ—¥æœŸ'], as_index=False
        )['æ¨å¹¿é€šèŠ±è´¹'].sum()

        # æŠŠæ¨å¹¿é€šèŠ±è´¹åˆå¹¶å›df_monthï¼Œç¼ºå¤±æ—¶è®¾ä¸º0
        df_month = df_month.merge(
            cpc_month[['brand_name','æ—¥æœŸ','æ¨å¹¿é€šèŠ±è´¹']],
            on=['brand_name','æ—¥æœŸ'], how='left'
        ).fillna({'æ¨å¹¿é€šèŠ±è´¹': 0})
        df_month['æ¨å¹¿é€šèŠ±è´¹'] = df_month['æ¨å¹¿é€šèŠ±è´¹'].round(2)

        # è¡ç”Ÿ"æ˜ŸæœŸ"ã€"è®¿é—®è½¬åŒ–"ã€"è´­ä¹°è½¬åŒ–"
        weekday_map = {0:'æ˜ŸæœŸä¸€',1:'æ˜ŸæœŸäºŒ',2:'æ˜ŸæœŸä¸‰',3:'æ˜ŸæœŸå››',4:'æ˜ŸæœŸäº”',5:'æ˜ŸæœŸå…­',6:'æ˜ŸæœŸæ—¥'}
        df_month['æ˜ŸæœŸ'] = pd.to_datetime(df_month['æ—¥æœŸ']).dt.weekday.map(weekday_map)
        df_month['è®¿é—®è½¬åŒ–'] = (df_month['è®¿é—®äººæ•°']/df_month['æ›å…‰äººæ•°']).round(3)
        df_month['è´­ä¹°è½¬åŒ–'] = (df_month['è´­ä¹°äººæ•°']/df_month['è®¿é—®äººæ•°']).round(3)

        # å®šä¹‰åˆ—é¡ºåº
        cols = [
            'æ—¥æœŸ','æ˜ŸæœŸ','æ¶ˆè´¹é‡‘é¢','æ¨å¹¿é€šèŠ±è´¹','æ›å…‰äººæ•°','è®¿é—®äººæ•°','è´­ä¹°äººæ•°',
            'è®¿é—®è½¬åŒ–','è´­ä¹°è½¬åŒ–','æ–°å¢æ”¶è—äººæ•°','æ‰“å¡äººæ•°',
            'æ–°å¥½è¯„æ•°','æ–°ä¸­å·®è¯„æ•°','æ‰«ç äººæ•°','ç‚¹è¯„æ˜Ÿçº§'
        ]

        # æŒ‰è¿è¥å¸ˆè¾“å‡ºæœˆåº¦Excelï¼ˆæ¯åº—ä¸€ä¸ªsheetï¼‰
        for op, grp_op in df_month.groupby('operator'):
            file = out_dir / f"{op}_{report_date.date()}_æœˆåº¦æ•°æ®.xlsx"
            
            # å†™æ¯ä¸ªé—¨åº—åˆ†åˆ«ä¸€ä¸ªsheet
            with pd.ExcelWriter(file, engine='openpyxl', mode='w') as writer:
                for brand, grp in grp_op.groupby('brand_name'):
                    sheet = grp.sort_values('æ—¥æœŸ')[cols]
                    sheet.to_excel(
                        writer,
                        sheet_name=brand[:31],  # sheetåç§°æˆª31å­—
                        index=False,
                        startrow=2
                    )
            logger.info(f"å·²ç”Ÿæˆè¿è¥å¸ˆæœˆåº¦Excelï¼š{file}")

            # æ ·å¼è®¾ç½®
            try:
                import openpyxl
                from openpyxl.styles import Font, PatternFill, Alignment
                from openpyxl.utils import get_column_letter
                
                wb = openpyxl.load_workbook(file)
                header_fill = PatternFill("solid", fgColor="DDDDDD")
                
                for ws in wb.worksheets:
                    max_col = ws.max_column

                    # åˆå¹¶ç¬¬1è¡Œå†™é—¨åº—åç§°
                    ws.merge_cells(start_row=1, start_column=1, end_row=1, end_column=max_col)
                    title = ws.cell(row=1, column=1)
                    title.value = ws.title
                    title.font = Font(size=14, bold=True)
                    title.alignment = Alignment(horizontal="center", vertical="center")

                    # ç¬¬3è¡Œæ ·å¼ï¼šåŠ ç²—å±…ä¸­ï¼‹ç°åº•
                    for col_idx in range(1, max_col + 1):
                        cell = ws.cell(row=3, column=col_idx)
                        cell.font = Font(bold=True)
                        cell.fill = header_fill
                        cell.alignment = Alignment(horizontal="center", vertical="center")

                    # å†»ç»“å‰ä¸‰è¡Œ
                    ws.freeze_panes = "A4"

                    # åˆ—å®½è‡ªé€‚åº” & å¼ºåˆ¶æœ€å°å®½åº¦
                    min_widths = {
                        'A': 13,  # æ—¥æœŸ
                        'B': 8,   # æ˜ŸæœŸ
                        'C': 10,  # æ¶ˆè´¹é‡‘é¢
                        'D': 10, 'E': 10, 'F': 10, 'G': 10,
                        'H': 10, 'I': 13, 'J': 10, 'K': 10, 'L': 11,
                        'M': 8,   # æ‰«ç 
                        'N': 8    # æ˜Ÿçº§
                    }
                    
                    for col_cells in ws.columns:
                        col_letter = get_column_letter(col_cells[0].column)
                        if col_letter == 'A':
                            ws.column_dimensions[col_letter].width = min_widths['A']
                            continue

                        max_len = max(
                            len(str(c.value)) if c.value is not None else 0
                            for c in col_cells
                        )
                        optimal = max_len + 4
                        ws.column_dimensions[col_letter].width = max(
                            optimal, min_widths.get(col_letter, optimal)
                        )

                    # å•å…ƒæ ¼æ ¼å¼åŒ–
                    fmt_amt2 = '#,##0.00'  # ä¸¤ä½å°æ•°
                    fmt_int = '#,##0'      # æ•´æ•°
                    pct_fmt = '0.0%'       # ç™¾åˆ†æ¯”
                    star_fmt = '0.0'       # æ˜Ÿçº§ä¸€ä½å°æ•°
                    
                    for row in range(4, ws.max_row + 1):
                        # Cåˆ—ï¼šæ¶ˆè´¹é‡‘é¢ä¿ç•™ä¸¤ä½å°æ•°
                        ws.cell(row, 3).number_format = fmt_amt2
                        # Dåˆ—ï¼šæ¨å¹¿é€šèŠ±è´¹ä¿ç•™ä¸¤ä½å°æ•°
                        ws.cell(row, 4).number_format = fmt_amt2
                        # E~Gåˆ—ï¼šæ›å…‰/è®¿é—®/è´­ä¹°ç”¨æ•´æ•°
                        ws.cell(row, 5).number_format = fmt_int
                        ws.cell(row, 6).number_format = fmt_int
                        ws.cell(row, 7).number_format = fmt_int
                        # Håˆ—ï¼šè®¿é—®è½¬åŒ–ç™¾åˆ†æ¯”
                        ws.cell(row, 8).number_format = pct_fmt
                        # Iåˆ—ï¼šè´­ä¹°è½¬åŒ–ç™¾åˆ†æ¯”
                        ws.cell(row, 9).number_format = pct_fmt
                        # æœ€åä¸€åˆ—ï¼ˆç‚¹è¯„æ˜Ÿçº§ï¼‰ç”¨ä¸€ä½å°æ•°
                        ws.cell(row, ws.max_column).number_format = star_fmt

                wb.save(file)
                logger.info(f"Excelå·²æ ¼å¼åŒ–ï¼š{file}")
                
            except ImportError:
                logger.warning("openpyxlæœªå®‰è£…ï¼Œè·³è¿‡Excelæ ·å¼è®¾ç½®")
            except Exception as e:
                logger.error(f"Excelæ ·å¼è®¾ç½®å¤±è´¥: {e}")

        logger.info("æ—¥æŠ¥ç”Ÿæˆå®Œæˆ")
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ—¥æŠ¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

def main():
    """ä¸»å‡½æ•°"""
    logger = get_logger('generate_daily_report')
    
    # é»˜è®¤ä½¿ç”¨æ˜¨å¤©çš„æ—¥æœŸ
    default_dt = datetime.now() - timedelta(days=1)
    default_str = default_dt.strftime("%Y-%m-%d")
    
    # äº¤äº’å¼æ—¥æœŸé€‰æ‹©
    import sys
    if len(sys.argv) > 1:
        date_str = sys.argv[1]
    else:
        # äº¤äº’å¼è¾“å…¥æ—¥æœŸ
        inp = input(f"ä½¿ç”¨æ—¥æœŸ [{default_str}]ï¼Ÿå›è½¦ç¡®è®¤ æˆ– è¾“å…¥å…¶ä»–æ—¥æœŸ (YYYY-MM-DD)ï¼š").strip()
        date_str = inp if inp else default_str
    
    try:
        report_date = datetime.strptime(date_str, "%Y-%m-%d")
    except ValueError:
        logger.error(f"æ—¥æœŸæ ¼å¼é”™è¯¯ï¼š{date_str}ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        return
    
    logger.info(f"ğŸ—“ï¸ æœ€ç»ˆä½¿ç”¨æ—¥æœŸï¼š{report_date.date()}ï¼Œç”Ÿæˆæ—¥æŠ¥")
    logger.info(f"æ—¥æŠ¥å°†è¾“å‡ºåˆ°: {REPORT_OUTPUT_DIR}")
    
    generate_daily_report(report_date, REPORT_OUTPUT_DIR)
    logger.info("æ—¥æŠ¥æµç¨‹å·²å®Œæˆ")

if __name__ == '__main__':
    main()
