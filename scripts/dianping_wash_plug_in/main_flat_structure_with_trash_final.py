import os
import pandas as pd
import json
import shutil
from sqlalchemy import create_engine, text, inspect
from config import DB_CONNECTION_STRING
from data_cleaning import (
    clean_operation_data, clean_numeric_columns, drop_percentage_columns,
    match_store_id_for_single_cpc, process_cpc_dates, add_datetime_column
)
from column_mappings import COLUMN_MAPPING_CPC_HOURLY
from excel_header_finder import clean_and_load_excel
from database_importer import import_to_mysql, get_dtype_for_operation, get_dtype_for_cpc_hourly
import send2trash  # âœ… æ·»åŠ å›æ”¶ç«™ä¾èµ–

engine = create_engine(DB_CONNECTION_STRING)

CPC_HOURLY_FOLDER = r"D:\dianping_downloads\cpc_hourly_data"
OPERATION_FOLDER  = r"D:\dianping_downloads\operation_data"

def delete_old_cpc_for_store(start_date, end_date, store_ids):
    with engine.begin() as conn:
        for sid in store_ids:
            res = conn.execute(
                text("DELETE FROM cpc_hourly_data WHERE store_id = :sid AND DATE(date) BETWEEN :s AND :e"),
                {"sid": sid, "s": start_date, "e": end_date}
            )
            print(f"âœ… åˆ é™¤ cpc_hourly_data ä¸­ é—¨åº—ID={sid} {start_date}~{end_date} å…± {res.rowcount} æ¡ã€‚")

def delete_old_op_for_store(start_date, end_date, store_ids):
    with engine.begin() as conn:
        for sid in store_ids:
            res = conn.execute(
                text("DELETE FROM operation_data WHERE `ç¾å›¢é—¨åº—ID` = :sid AND DATE(`æ—¥æœŸ`) BETWEEN :s AND :e"),
                {"sid": sid, "s": start_date, "e": end_date}
            )
            print(f"âœ… åˆ é™¤ operation_data ä¸­ é—¨åº—ID={sid} {start_date}~{end_date} å…± {res.rowcount} æ¡ã€‚")

def build_rankings_detail(row):
    city = str(row.get("åŸå¸‚", "")).strip()
    cols = {
        "ç¾å›¢äººæ°”æ¦œæ¦œå•æ’å":   "meituan_popularity",
        "ç¾å›¢å¥½è¯„æ¦œæ¦œå•æ’å":   "meituan_rating",
        "ç‚¹è¯„çƒ­é—¨æ¦œæ¦œå•æ’å":   "dianping_hot",
        "ç‚¹è¯„å¥½è¯„æ¦œæ¦œå•æ’å":   "dianping_rating",
        "ç‚¹è¯„å£å‘³æ¦œæ¦œå•æ’å":   "dianping_taste",
        "ç‚¹è¯„ç¯å¢ƒæ¦œæ¦œå•æ’å":   "dianping_env",
        "ç‚¹è¯„æœåŠ¡æ¦œæ¦œå•æ’å":   "dianping_service",
        "ç‚¹è¯„æ‰“å¡äººæ°”æ¦œæ¦œå•æ’å": "dianping_checkin",
    }
    detail = {}
    for src_col, key in cols.items():
        raw = row.get(src_col)
        if not raw or pd.isna(raw):
            continue
        tmp = {}
        for part in str(raw).split("|"):
            import re
            m = re.search(r'(.+?)ç¬¬(\d+)å', part.strip())
            if not m:
                continue
            scope = m.group(1).strip()
            rank  = int(m.group(2))
            if city and city in scope:
                tmp['city'] = rank
            elif scope.endswith('åŒº'):
                tmp['district'] = rank
            else:
                tmp['subdistrict'] = rank
        if tmp:
            detail[key] = tmp
    return json.dumps(detail, ensure_ascii=False)

def process_operation_folder():
    print(f"ğŸ“‚ æ­£åœ¨è¯»å–è¿è¥æ•°æ®è·¯å¾„ï¼š{OPERATION_FOLDER}")
    dfs = []
    filepaths = []

    for fname in os.listdir(OPERATION_FOLDER):
        if not fname.endswith(".xlsx"):
            continue
        fp = os.path.join(OPERATION_FOLDER, fname)
        df = clean_operation_data(clean_and_load_excel(fp))
        df = drop_percentage_columns(df)
        # æ¸…æ´—æ•°å€¼åˆ—ï¼Œé‡åˆ°å¼‚å¸¸é¡¹ä¼šæ‰“å°å‡ºç¾å›¢é—¨åº—ID
        df = clean_numeric_columns(df, key_col='ç¾å›¢é—¨åº—ID')
        dfs.append(df)
        filepaths.append(fp)

    if not dfs:
        print("âš ï¸ æ— è¿è¥æ•°æ®æ–‡ä»¶å¯¼å…¥")
        return

    df_all = pd.concat(dfs, ignore_index=True)
    df_all.drop_duplicates(subset=["æ—¥æœŸ", "ç¾å›¢é—¨åº—ID"], keep="last", inplace=True)
    df_all["æ—¥æœŸ"] = pd.to_datetime(df_all["æ—¥æœŸ"]).dt.date
    min_date, max_date = df_all["æ—¥æœŸ"].min(), df_all["æ—¥æœŸ"].max()
    store_ids = df_all["ç¾å›¢é—¨åº—ID"].astype(str).unique().tolist()

    delete_old_op_for_store(min_date, max_date, store_ids)

    inspector = inspect(engine)
    existing_cols = [c["name"] for c in inspector.get_columns("operation_data")]
    flat_cols = [c for c in existing_cols if c in df_all.columns]
    df_basic = df_all[flat_cols].copy()

    dynamic_df = df_all.drop(columns=flat_cols, errors="ignore")
    dicts = dynamic_df.to_dict(orient="records")
    cleaned = [{k: (None if pd.isna(v) else v) for k, v in d.items()} for d in dicts]
    df_basic["extra_metrics"] = [json.dumps(d, ensure_ascii=False) for d in cleaned]

    if "ROSåˆ†" in df_all.columns:
        df_basic["ros_score"] = pd.to_numeric(df_all["ROSåˆ†"], errors="coerce").fillna(0).astype(int)

    df_basic["rankings_detail"] = df_all.apply(build_rankings_detail, axis=1)

    dtype_op = get_dtype_for_operation(df_basic)
    import_to_mysql(df_basic, "operation_data", DB_CONNECTION_STRING, dtype=dtype_op)
    print(f"âœ… æˆåŠŸå¯¼å…¥è¿è¥æ•°æ®ï¼Œå…± {len(df_basic)} è¡Œã€‚")

    for fp in filepaths:
        send2trash.send2trash(fp)
        print(f"ğŸ—‘ï¸ å·²ç§»å…¥å›æ”¶ç«™ï¼š{fp}")

def process_cpc_folder():
    print(f"ğŸ“‚ æ­£åœ¨è¯»å–å°æ—¶çº§CPCæ•°æ®è·¯å¾„ï¼š{CPC_HOURLY_FOLDER}")
    store_mapping = pd.read_sql("SELECT * FROM store_mapping", con=engine)
    dfs = []
    filepaths = []

    for fname in os.listdir(CPC_HOURLY_FOLDER):
        if not fname.endswith(".xlsx"):
            continue
        fp = os.path.join(CPC_HOURLY_FOLDER, fname)
        df = clean_and_load_excel(fp)
        df = process_cpc_dates(df, fname)
        df = match_store_id_for_single_cpc(df, store_mapping)
        df = drop_percentage_columns(df)
        # æ¸…æ´—æ•°å€¼åˆ—ï¼Œé‡åˆ°å¼‚å¸¸é¡¹ä¼šæ‰“å°å‡ºé—¨åº—åç§°
        df = clean_numeric_columns(df, key_col='é—¨åº—åç§°')
        df = add_datetime_column(df)
        dfs.append(df)
        filepaths.append(fp)

    if not dfs:
        print("âš ï¸ æ— CPCæ•°æ®æ–‡ä»¶å¯¼å…¥")
        return

    df_all = pd.concat(dfs, ignore_index=True)
    df_all['plan_key'] = (
        df_all['é—¨åº—ID'].astype(str).str.strip()
        + "_"
        + df_all['æ¨å¹¿åç§°'].astype(str).str.strip()
        + "_"
        + df_all['å¹³å°'].astype(str).str.strip()
    )
    df_all.rename(columns=COLUMN_MAPPING_CPC_HOURLY, inplace=True)
    df_all.drop_duplicates(subset=['plan_key', 'start_time'], inplace=True)

    df_all['date'] = pd.to_datetime(df_all['date']).dt.date
    min_date, max_date = df_all['date'].min(), df_all['date'].max()
    store_ids = df_all['store_id'].astype(str).unique().tolist()
    delete_old_cpc_for_store(min_date, max_date, store_ids)

    dtype_cpc = get_dtype_for_cpc_hourly(df_all)
    import_to_mysql(df_all, "cpc_hourly_data", DB_CONNECTION_STRING, dtype=dtype_cpc)
    print(f"âœ… æˆåŠŸå¯¼å…¥å°æ—¶çº§CPCæ•°æ®ï¼Œå…± {len(df_all)} è¡Œã€‚")

    for fp in filepaths:
        send2trash.send2trash(fp)
        print(f"ğŸ—‘ï¸ å·²ç§»å…¥å›æ”¶ç«™ï¼š{fp}")

if __name__ == "__main__":
    process_operation_folder()
    process_cpc_folder()
